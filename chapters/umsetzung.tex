
% ===========================
\chapter{Umsetzung}
\label{umsetzung}
% ===========================

In diesem Kapitel wird die Umsetzung des Konzepts aus dem vorherigen Kapitel beschrieben. Zu Beginn werden die dafür notwendigen Fahrszenarien in Abschnitt \ref{umsetzung_definition} definiert. Danach wirda in den Abschnitten \ref{umsetzung_daten_synth} und \ref{umsetzung_daten_real} die Methodik für die Generierung von synthetische Trainingsdaten und reale Trainings- und Testdaten erläutert. Im Anschluss wird in Abschnitt \ref{umsetzung_training} die Architektur und die Experimente des Klassifikators für die Szenarienerkennung beschrieben.


% ===========================
\section{Definition der Fahrszenarien}
\label{umsetzung_definition}
% ===========================

In diesem Abschnitt werden Szenarien, wie in Abschnitt \ref{grundlagen_fahren_szenarien} beschrieben, als \textit{logische Szenarien} für das weitere Vorgehen in dieser Arbeit definiert. In Anlehnung an bestehende Arbeiten zu Erkennung von Fahrszenarien und auf Basis von Machbarkeitsabschätzungen für die Umsetzung werden in dieser Arbeit die Szenarien \textit{free cruising}, \textit{approaching} \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right} auf der Autobahn betrachtet. Die Autobahn wurde ausgewählt, weil es weniger Parameter zu betrachten gibt als auf anderen Straßen wie beispielsweise in der Stadt. In der folgenden Tabelle \ref{tab_definition_szenarios} werden diese Szenarien auf \textit{funktionaler} und \textit{logischer Ebene} definiert.

Um die Darstellung in der Tabelle zu erleichtern werden folgende Abstände zwischen Ego-Fahrzeug und Fahrzeug 2 definiert. Dabei beschreibt $ego_v$ die Geschwindigkeit des Ego-Fahrzeugs in [m/s].

\begin{equation*}
\begin{split}
s_0 = ego_v * 3,6 \qquad \text{[m]} \\
s_1 = ego_v * 3,6 * \frac{2}{3} \qquad \text{[m]} \\
s_2 = ego_v * 3,6 * \frac{1}{2} \qquad \text{[m]} \\
s_3 = ego_v * 3,6 * \frac{1}{3} \qquad \text{[m]} \\
\end{split}
\end{equation*}

\small
\begin{longtable}[c]{p{2cm} p{5.5cm} p{6cm}}
\textbf{Szenario} & \textbf{Funktionale Definition} & \textbf{Logische Definition} \\
\hline
\endhead

\textbf{Alle} & 2-spurige Autobahn geradeaus oder in einer Kurve, Geschwindigkeitsbegrenzung ist größer als 80 km/h & Breite Fahrstreifen [2,3..3,5] m \newline Geschwindigkeitsbegrenzung [80..keine] km/h \\
\hline

\textbf{Alle} & Tageslicht, keine Wolken bis leicht bewölkt, kein Niederschlag, gute Sichtbedingungen & Tageszeit [Sonnenaufgang..Sonnenuntergang] \newline Bewölkung [leicht bewölkt..wolkenlos]\\
\hline \hline

\textbf{Free cruising} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt frei auf linker oder rechter Fahrspur, andere Fahrzeuge sind weit entfernt und haben keinen Einfluss auf die Manöver des Ego & Geschwindigkeit Ego [60..200] km/h \newline Abstand zu anderen Verkehrsteilnehmern [$>s_0$] m \\
\hline

\textbf{Approaching} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego nähert sich auf linker oder rechter Fahrspur in mittlerem Abstand dem Fahrzeug 2 & Geschwindigkeit Ego abnehmend [60..200] km/h \newline Geschwindigkeit Ego < Geschwindigkeit Fahrzeug 2 \newline Abstand Ego zu Fahrzeug 2 [$s_2$..$s_0$] m \\
\hline

\textbf{Following} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt auf linker oder rechter Fahrspur in sicherem Abstand hinter Fahrzeug 2 & Geschwindigkeit Ego [60..200] km/h \newline Geschwindigkeitsdifferenz zwischen Ego und Fahrzeug 2 < Geschwindigkeit Ego $*0,05$ km/h \newline Abstand Ego zu Fahrzeug 2 [$s_3$..$s_1$] m \newline Ego befindet sich auf gleicher Fahrspur hinter Fahrzeug 2 \\
\hline

\textbf{Catching up} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt auf linker Fahrspur und verringert den vertikalen Abstand zu Fahrzeug 2 auf der rechten Fahrspur (auf-/überholen) & Geschwindigkeit Ego [60..200] km/h \newline Geschwindigkeit Fahrzeug 2 < Geschwindigkeit Ego \newline Vertikaler Abstand Ego zu Fahrzeug 2 [0..$s_0$] m \newline Ego fährt auf linker Fahrspur hinter Fahrzeug 2 das auf rechter Fahrspur fährt \\
\hline

\textbf{Overtaking} & & \\
\hline

\textbf{Lane change left} & Ego, andere Verkehrsteilnehmer sind optional \newline \underline{Interaktion:} Ego fährt auf rechter Fahrspur und wechselt auf linke Fahrspur & Geschwindigkeit Ego [60..200] km/h \newline Ego befindet sich auf rechter Fahrspur und wechselt auf linke Fahrspur \\
\hline

\textbf{Lane change right} & Ego, andere Verkehrsteilnehmer sind optional \newline \underline{Interaktion:} Ego fährt auf linker Fahrspur und wechselt auf rechte Fahrspur & Geschwindigkeit Ego [60..200] km/h \newline Ego befindet sich auf linker Fahrspur und wechselt auf rechte Fahrspur \\
\hline

\caption{Definition der Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right}}
\label{tab_definition_szenarios}
\end{longtable}
\normalsize

% ===========================
\section{Generierung synthetischer Trainingsdaten}
\label{umsetzung_daten_synth}
% ===========================

Auf Basis der Definitionen aus dem vorherigen Abschnitt \ref{umsetzung_definition} werden in diesem Abschnitt die benötigten Signal- und Bilddaten simuliert und entsprechend gelabelt. Dafür werden in Abschnitt \ref{umsetzung_daten_synth_simulation} die Signaldaten, die für die eindeutige Klassifizierung der Szenarien benötigt werden, simuliert. In Abschnitt \ref{umsetzung_daten_synth_labeling} werden diese Signaldaten verwendet um die parallel simulierten Bilddaten entsprechend zu labeln.
 
% ===========================
\subsection{Simulation mit CarMaker}
\label{umsetzung_daten_synth_simulation}
% ===========================

Für die Simulation der Signal- und Bilddaten wird die kommerzielle Software CarMaker von IPG Automotive \cite{ipg2018carmaker} verwendet. Diese Simulationssoftware wird für den virtuellen Fahrversuch und \ac{HiL}-Tests eingesetzt um Komponenten in unterschiedlichen Szenarien zu testen. In dieser Arbeit wird CarMaker verwendet, um die Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right} zu simulieren.

Für die Aufnahme der benötigten Bilddaten wird im simulierten Fahrzeug ein entsprechender Kamerasensor konfiguriert. Die Konfiguration des Sensors orientiert sich an der Konfiguration von realen Frontview-Kameras im Fahrzeug nach Punke \cite{punke2015kamera}. So ist der Kamerasensor an der Stelle des Rückfahrspiegels platziert und hat eine Auflösung von 640 x 480 Pixeln und ein Sichtfeld von 20°.

Die benötigten Signaldaten für das Labeln werden von den Definitionen aus Abschnitt \ref{umsetzung_definition} abgeleitet. Für die eindeutige Identifikation der logischen Szenarien werden die folgenden Werte benötigt: Geschwindigkeit des Ego-Fahrzeugs, Abstand und Geschwindigkeitsdifferenz des Ego-Fahrzeugs zu allen anderen Fahrzeugen, aktuelle Fahrspur des Ego-Fahrzeugs und allen anderen Fahrzeugen und die relative Position des Ego-Fahrzeugs, i.e. ob sich das Ego-Fahrzeug vor oder hinter einem anderen Fahrzeug befindet. Um den Abstand und die Geschwindigkeitsdifferenz des Ego-Fahrzeugs zu allen anderen Fahrzeugen aufzuzeichnen, wird ein Objektsensor im Ego-Fahrzeug konfiguriert. Mit diesem Sensor können im konfigurierten Radius alle Fahrzeuge und ihr Abstand und ihre relative Geschwindigkeit zum Ego-Fahrzeug erfasst und über die \textit{OutputQuantities} in CarMaker aufgezeichnet werden. Die Geschwindigkeit des Ego-Fahrzeugs und die Fahrspur-ID und Position aller Fahrzeuge können direkt, ohne zusätzlichen Sensor, über die \textit{OutputQuantities} aufgezeichnet werden. Die jeweiligen Variablen in CarMaker sind in der Tabelle \ref{tab_output_quantities} zusammengefasst.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{p{6.2cm} p{7.5cm}}
\textbf{Variable in CarMaker} & \textbf{Beschreibung} \\
\hline

Car.v & Geschwindigkeit des Ego-Fahrzeugs in [m/s] \\
Car.Road.sRoad & Position des Ego-Fahrzeugs auf der Strecke in [m] \\
Car.Road.Lane.Act.LaneId & Fahrspur-ID des Ego-Fahrzeugs \\
\hline
Sensor.Object.OB01.TX.NearPnt.dv\_p & Geschwindigkeitsdifferenz zwischen Fahrzeug TX und dem Ego-Fahrzeug in [m/s] \\
Sensor.Object.OB01.TX.NearPnt.ds\_p & Abstand zwischen Fahrzeug TX und dem Ego-Fahrzeug in [m] \\
\hline
Traffic.TX.sRoad & Position des Fahrzeugs TX auf der Strecke in [m] \\
Traffic.TX.Lane.Act.LaneId & Fahrspur-ID des Fahrzeugs TX \\
\hline

\end{tabular}
\caption{Aufgezeichnete Signaldaten in CarMaker}
\label{tab_output_quantities}
\end{table}

Für die Simulation werden zwei Strecken der Länge 6.000m und 10.000m mit dem \textit{CarMaker - Scenario Editor} erstellt. Bei beiden Strecken handelt es sich um eine 4-spurige Autobahn mit zwei Fahrspuren in jede Richtung. Die Fahrtrichtungen sind in der Mitte von einer Leitplanke getrennt und am Rand der Fahrbahn sind jeweils Standstreifen vorhanden. Abschnittsweise stehen neben der Fahrbahn auch einige Bäume, was in Abbildung \ref{fig_cm_road_strecke} mit grünen Streifen gekennzeichnet ist. Abbildung \ref{fig_cm_road_bild} zeigt die Konfiguration der simulierten Straße.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{cm_road_bild.png}
\caption{Konfiguration der simulierten Straße \cite{ipg2018carmaker}}
\label{fig_cm_road_bild}
\end{figure}

Auf beiden Strecken wird autonomer, stochastisch verteilter Verkehr erzeugt, was CarMaker mit einer gesonderten Funktion unterstützt. Der Verkehr wird in einer niedrigen Dichte (10\%) und einem 80\%-igen Anteil Autos erzeugt, andere Fahrzeuge sind Motorräder, Lastkraftwagen und Busse. In CarMaker ist eine Vielzahl an unterschiedlichen Fahrzeugen verfügbar, was wichtig ist um möglichst viele unterschiedliche Szenarien zu generieren. Mit dieser Konfiguration werden auf der 10.000m-Strecke 131 Fahrzeuge und auf der 6.000m-Strecke 89 Fahrzeuge generiert.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Strecke 1]{\includegraphics[scale=0.8]{cm_road_1.png}} \\
\subfloat[Strecke 2]{\includegraphics[scale=0.8]{cm_road_2.png}}
\end{tabular}
\caption{Schema der simulierten Strecken 1 und 2 \cite{ipg2018carmaker}}
\label{fig_cm_road_strecke}
\end{figure}

Die Simulation und Generierung von Bild- und Signaldaten wird mit dem \textit{CarMaker - Test Manager} durchgeführt. Mit diesem Modul lassen sich Fahrten mit unterschiedlichen Konfigurationen simulieren. In dieser Arbeit werden die Variablen \textit{Geschwindigkeit}, \textit{Mindestabstand zu vorausfahrendem Fahrzeug}, \textit{Minimale Geschwindigkeitsdifferenz beim Überholen} und \textit{Aggressivität beim Überholen} auf beiden oben beschriebenen Strecken variiert. Die Werte der Variablen, die simuliert werden, sind in Tabelle \ref{tab_tm_variablen} aufgelistet und sind aus der Sicht des Ego-Fahrzeugs.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{p{8cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm}}
\textbf{Variable} & \textbf{Werte} & & & & \\
\hline
Geschwindigkeit in [km/h] & 100 & 120 & 140 & 160 & 180 \\
Mindestabstand zu vorausfahrendem Fahrzeug in [s] & 1,0 & 1,5 & 2,0 & & \\
Minimale Geschwindigkeitsdifferenz beim Überholen in [km/h] & 5 & 15 & 25 & & \\
Aggressivität beim Überholen & 0,2 & 0,6 & 1,0 & & \\
\hline
\end{tabular}
\caption{Variablen und Werte die in der Simulation verwendet werden}
\label{tab_tm_variablen}
\end{table}

Die ersten drei Variablen sind selbsterklärend und werden hier nicht weiter erläutert. Die Variable \textit{Aggressivität beim Überholen} (in CarMaker \textit{Overtaking Rate} ist eine Zahl zwischen 0 und 1. Dabei markiert die 0 ein Fahrstil, bei dem sich der Fahrer sehr risikoavers beim Überholen verhält, i.e. Überholen nur in sehr sicheren Situationen. Je größer die Zahl wird, desto aggressiver wird der Überholvorgang und dementsprechend sinkt die Risikoaversion beim Überholen und der Fahrer überholt auch bei kritischen oder schlecht einsehbaren Situationen.

Mit diesen vier Variablen mit jeweils drei bzw. fünf Werten ergeben sich 135 verschiedene Kombinationsmöglichkeiten. Somit werden auf beiden Strecken in Summe 270 Fahrten mit 2.160 km simuliert. Signal- und Bilddaten werden mit einer Frequenz von 5 Hz aufgezeichnet, was in 326.108 aufgezeichneten Szenen (Bilder und Signaldaten) resultiert. Diese Szenen werden im folgenden Abschnitt \ref{umsetzung_daten_synth_labeling} gelabelt.


% ===========================
\subsection{Daten Labeling}
\label{umsetzung_daten_synth_labeling}
% ===========================

Für das Labeln der Szenarien wird jede Szene auf Basis der Definition aus Abschnitt \ref{umsetzung_definition} mithilfe der Signaldaten klassifiziert. Die logischen Bedingungen für jedes Szenario sind dafür in Tabelle \ref{tab_szenarien_labeling} aufgelistet. Auf Basis der CarMaker-Variablen aus Tabelle \ref{tab_tm_variablen} werden folgende zusätzliche Variablen definiert, um nachfolgende Bedingungen übersichtlicher darzustellen. Dabei beschreibt $v2$ jeweils das Fahrzeug, auf Basis dessen das jeweilige Szenario klassifiziert wird.

\begin{equation*}
\begin{split}
ego_v = \text{Car.v} \qquad \text{[m/s]} \\
ego_{sRoad} = \text{Car.Road.sRoad} \qquad \text{[m]} \\
ego_{laneID} = \text{Car.Road.Lane.Act.LanId} \qquad \text{[1, 2]} \\
v2_{dv} = \text{Sensor.Object.OB01.TX.NearPnt.dv\_p} \qquad \text{[m/s]} \\
v2_{ds} = \text{Sensor.Object.OB01.TX.NearPnt.dv\_p} \qquad \text{[m]} \\
v2_{sRoad} = \text{Traffic.TX.sRoad} \qquad \text{[m]} \\
v2_{laneID} = \text{Traffic.TX.Lane.Act.LaneId} \qquad \text{[1, 2]} \\
\end{split}
\end{equation*}

\small
\begin{longtable}[c]{p{3cm} p{8.5cm}}
\textbf{Szenario} & \textbf{Bedingungen} \\
\hline
\endhead

\textbf{Free cruising} & $ego_v > 17$ \newline $s_0 < v2_{ds}$ \\
\hline
\textbf{Approaching} & $s_2 < v2_{ds} < s_0$ \newline $ego_{sRoad} < v2_{sRoad}$ \newline $ego_{laneID} == v2_{laneID}$ \newline $ego_v < \text{ Durchschnitt von } ego_v \text{ der letzten 3 Sekunden} $ \\
\hline
\textbf{Following} & $v2_{dv} < ego_v * 0,05$ \newline $s_3 < s_1$ \newline $ego_{sRoad} < v2_{sRoad}$ \newline $ego_{laneID} == v2_{laneID}$ \\
\hline
\textbf{Catching up} & $v2_{dv} < 0$ \newline $0 <= v2_{ds} < s_0$ \newline $ego_{sRoad} <= v2_{sRoad}$ \newline $ego_{laneID} == v2_{laneID} - 1$ \\
\hline
\textbf{Overtaking} & $0 <= v2_{ds} < s_0$ \newline $v2_{sRoad} < ego_{sRoad}$ \newline $ego_{laneID} == v2_{laneID} - 1$ \\
\hline
\textbf{Lane change left} & $ego_{laneID}^{before} == ego_{laneID}^{after} + 1$ \newline Als Spurwechsel wird ein Intervall von 4 Sekunden betrachtet in dessen Mitte die Variable ihren Wert wechseln muss\\
\hline
\textbf{Lane change right} & $ego_{laneID}^{before} == ego_{laneID}^{after} - 1$ \newline Als Spurwechsel wird ein Intervall von 4 Sekunden betrachtet in dessen Mitte die Variable ihren Wert wechseln muss \\

\hline
\caption{Bedingungen der Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right}}
\label{tab_szenarien_labeling}
\end{longtable}
\normalsize

Im Anschluss an die Klassifizierung einzelner Zeitpunkte werden diese zu Szenarien zusammengefasst, wenn mindestens 15 Zeitpunkte (3 Sekunden) in Folge mit dem gleichen Label klassifiziert wurden. Drei Sekunden wird den folgenden zwei Gründen als Länge für Szenarien in dieser Arbeit gewählt. Erstens orientiert sich diese Zeitspanne an den vorherigen Arbeiten zur Szenarienerkennung. Zweitens haben die zwei Szenarien \textit{lane change left} und \textit{lane change right} jeweils eine natürliche Länge von 3-4 Sekunden. Alle anderen Szenarien können länger sein, lassen sich aber in Blöcke von jeweils 3 Sekunden einteilen. Insgesamt werden 326.108 Zeitpunkte und 23.972 Szenarien klassifiziert. Die Anzahl der simulierten Szenarien nach Klasse ist in der Tabelle \ref{tab_verteilung_szenarien} dargestellt.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{l c}
\textbf{Szenario} & \textbf{Anzahl} \\
\hline
Free cruising & 2.545 \\
Approaching & 3.512 \\
Following & 3.601 \\
Catching up & 5.563 \\
Overtaking & 5.149 \\
Lane change left & 957 \\
Lane change right & 975 \\
Unknown & 1.670 \\
\hline
Summe & 23.972 \\
\hline
\end{tabular}
\caption{Anzahl der simulierten Szenarien nach Klasse}
\label{tab_verteilung_szenarien}
\end{table}

Es sind auch einige Szenarien als \textit{unknown} klassifiziert, da zwischen den definierten Szenarien andere Situationen auftreten können oder nicht die Mindestanzahl der konsekutiven Szenen erreicht wird. Zu manchen Zeitpunkten werden mehr als eine einzelne Szene klassifiziert. Beispielsweise kann sich das Ego-Fahrzeug gleichzeitig in der Szene \textit{catching up} und \textit{overtaking} befinden, wenn es auf der linken Fahrspur fährt und sich in vertikalem Abstand ein anderes Fahrzeug jeweils vor und hinter dem Ego-Fahrzeug befindet. Abbildung \ref{fig_beispiel_szenario_lcl} zeigt ein Beispiel des Szenarios \textit{lane change left} mit allen zugehörigen 15 Bildern.

\begin{figure}[h]
\centering
\begin{tabular}{c c c c c}
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl0.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl1.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl2.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl3.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl4.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl5.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl6.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl7.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl8.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl9.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl10.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl11.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl12.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl13.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcl_sim/lcl14.jpg}} \\
\end{tabular}
\caption{Beispiel eines simulierten Szenarios der Klasse \textit{lane change left} \cite{ipg2018carmaker}}
\label{fig_beispiel_szenario_lcl}
\end{figure}

% ===========================
\section{Generierung realer Trainings- und Testdaten}
\label{umsetzung_daten_real}
% ===========================

Für den Proof-of-Concept dieser Arbeit, die Erkennung von realen Fahrszenarien, werden neben den synthetischen Trainingsdaten auch reale Daten für das Training und die anschließenden Tests benötigt. Dafür werden im ersten Schritt bestehen Datensätze nach ihrer Nutzbarkeit untersucht.

Die bekanntesten Datensätze sind KITTI \cite{geiger2013vision}, BDD100K \cite{yu2018bdd100k}, Cityscapes \cite{cordts2016cityscapes} und Oxford RobotCar \cite{maddern20171}. Der Cityscapes und Oxford RobotCar Datensatz umfasst lediglich Bilder von Szenen in Städten und ist daher nicht nutzbar für diese Arbeit. Die Datensätze KITTI und BDD100K umfassen auch Videos von Autobahnfahrten, allerdings liegt der Fokus auf Objekterkennung in einzelnen Bildern oder semantischer Segmentation. Und es gibt jeweils nur sehr begrenzt Videos, die auf einer 2-spurigen Autobahn aufgenommen wurden. Daher können diese Datensätze in dieser Arbeit nicht verwendet werden.

Als Alternative werden die Aufnahmen von zwei Fahrten auf der Autobahn und Bundesstraße verwendet. Die Strecken sind in der Abbildung \ref{fig_strecken_real} abgebildet. 

\begin{figure}[h]
\centering
\begin{tabular}{c c}
\subfloat[Route 1: Mannheim - Rostock]{\includegraphics[scale=0.7]{route_1.png}} &
\subfloat[Route 2: Karlsruhe - Kandel]{\includegraphics[scale=0.7]{route_2.png}} \\
\end{tabular}
\caption{Routen für die Aufnahme der realen Bilddaten \cite{google2018route1, google2018route2}}
\label{fig_strecken_real}
\end{figure}

Die Aufnahme der Route 1 umfasst über sechs Stunden Videomaterial mit über einer Stunde Fahrt auf einer 2-spurigen Autobahn \cite{youtube2018video}. Davon werden manuell zwischen 50 und 180 Szenarien aus jeder Klasse gelabelt. Da von den Szenarien \textit{lane change left} und \textit{lane change right} jeweils nur 50 Szenarien vorhanden sind, wird vom Autor eine Fahrt auf Route 2 mit einigen Spurwechseln aufgenommen. Das Ergebnis sind jeweils 17 weitere Szenarien in den Klassen \textit{lane change left} und \textit{lane change right}.Abbildung \ref{fig_beispiel_szenario_lcr_real} zeigt ein Beispiel des realen Szenarios \textit{lane change right} mit allen zugehörigen 15 Bildern.

\begin{figure}[h]
\centering
\begin{tabular}{c c c c c}
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame0.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame1.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame2.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame3.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame4.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame5.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame6.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame7.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame8.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame9.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame10.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame11.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame12.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame13.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{lcr_real/frame14.jpg}} \\
\end{tabular}
\caption{Beispiel eines realen Szenarios der Klasse \textit{lane change right}}
\label{fig_beispiel_szenario_lcr_real}
\end{figure}

In Tabelle \ref{tab_datensaetze} wird dieser Datensatz, mit synthetischen und realen Daten, mit anderen synthetischen Datensätzen, die in Abschnitt \ref{grundlagen_nn_synthetisch} vorgestellt wurden, und realen Datensätzen, die in diesem Abschnitt vorgestellt werden, verglichen.

\small
\begin{longtable}[c]{p{2cm} p{3cm} p{3cm} p{4cm}}
\textbf{Datensatz} & \textbf{Generierung} & \textbf{Umfang} & \textbf{Label} \\
\hline
\endhead

\cite{ros2016synthia} & synthetisch mit der Unity Development Platform & 200.000 Bilder & semantische Annotation auf Pixelebene von 11 Klassen  \\
\hline
\cite{johnson2017driving} & synthetisch mit dem Computerspiel GTA5 & 250.000 Bilder & Begrenzungsboxen für die Klasse Fahrzeuge \\
\hline
\cite{richter2016playing} & synthetisch mit dem Computerspiel GTA5 & 25.000 Bilder & semantische Annotation auf Pixelebene von 19 Klassen \\
\hline
\cite{tremblay2018training} & synthetisch mit 3D Modelle von Fahrzeugen und realen Szenen & 100.000 Bilder & Begrenzungsboxen für die Klasse Fahrzeuge \\
\hline
KITTI, \cite{geiger2013vision} & real mit Fahrten in der Region Karlsruhe & 22 Videos und 15.000 Bilder & Begrenzungsboxen mit 8 Klassen \\
\hline
BDD100K, \cite{yu2018bdd100k} & real mit Fahrten in der Städten New York, Berkeley, San Francisco und der Bay Area & 100.000 Videos (40 Sekunden) mit insgesamt 120 Mio. Bilder & für jeweils das zehnte Bild aus jedem Video: Fahrbahnbegrenzungslinien und Begrenzungsboxen für 10 Objekte; Für 10.000 Bilder semantische Annotation auf Pixelebene von 19 Klassen \\
\hline
Cityscapes, \cite{cordts2016cityscapes} & real mit Fahrten in 50 Städten in Deutschland & 25.000 Bilder & semantische Annotation auf Pixelebene von 30 Klassen \\
\hline
Oxford RobotCar, \cite{maddern20171} & real mit Fahrten in Oxford & knapp 20 Mio. Bilder & keine Label \\
\hline
Diese Arbeit & synthetisch mit CarMaker und real mit zwei Autobahnfahrten in Deutschland & 24.307 Szenarien (3 Sekunden) mit insgesamt 331.133 Bilder & 7 Szenario-Klassen \\

\hline
\caption{Vergleich von synthetischen und realen Datensätzen}
\label{tab_datensaetze}
\end{longtable}
\normalsize


% ===========================
\section{Training}
\label{umsetzung_training}
% ===========================

In diesem Abschnitt wird zu Beginn das Format und der Import der Trainings- und Testdaten in das \ac{KNN} erklärt. Danach werden verschiedene Architekturen von \acp{KNN}, die in dieser Arbeit zum Einsatz kommen, vorgestellt und schließlich werden die durchgeführten Experimente mit diesen Architekturen erläutert. Die Vorbereitung der Daten und das Training wird mit Python und der Deep-Learning-Bibliothek Keras \cite{chollet2015keras} implementiert.


% ===========================
\subsection{Vorbereitung der Inputdaten}
\label{umsetzung_training_input}
% ===========================

Die Generierung der synthetischen und realen Trainings- und Testdaten wurde bereits in den vorherigen Abschnitten beschrieben. In diesem Abschnitt soll kurz darauf eingegangen werden wie diese Daten in das jeweilige \ac{KNN} eingespeist werden.

In dieser Arbeit werden \acp{CNN} für die Erkennung einzelner Bilder und Kombinationen aus \acp{CNN} und \ac{LSTM} für die Klassifizierung von Videos eingesetzt. Daher müssen sowohl einzelne Bilder, als auch Bildsequenzen in das jeweilige \ac{KNN} importiert werden. Da es sich um große Datenmengen von über 50GB handelt, müssen die Daten mit einem Datenstrom eingespeist werden, da nicht alle Daten zu Beginn in den Arbeitsspeicher geladen werden können.

Die Deep-Learning-Bibliothek Keras hat bereits sogenannte \textit{DataGenerators} für der Import von einzelnen Bildern und für sequenzielle Daten mit zwei Dimensionen (e.g. CSV-Dateien). Es gibt allerdings keine bereits nutzbare Lösung für den Import von sequentiellen Bildern, was 4-dimensionalen Daten entspricht (Anzahl Bilder, Höhe, Breite, Farbkanal). Aus diesem Grund wird die Lösung von Amidi \ref{amidi2017datagenerator} adaptiert um höher-dimensionale Datensets importieren zu können.

Von den generierten synthetischen und realen Szenarien werden \textit{free cruising}, \textit{following}, \textit{catching up}, \textit{lane change left} und \textit{lane change right} für das Training ausgewählt. Das Szenario \textit{approaching} wurde für den Proof-of-Concept in dieser Arbeit bewusst entfernt, weil es oft Überscheidungen mit dem Szenario \textit{following} gibt. Diese Überschneidungen und Ähnlichkeiten zwischen Szenarien und ihren Einfluss auf das Training eines Klassifikators können in weiteren Arbeiten untersucht werden. Das Szenario \textit{overtaking} wurde entfernt, weil es mit der konfigurierten Frontkamera allein nicht erfasst werden kann. In folgenden Arbeiten kann dieses Szenario mithilfe weiterer Kameraperspektiven zusätzlich berücksichtigt werden.

Um das Ergebnis nicht zu verzerren, sollten für das Training mit neuronalen Netzen in jeder Klasse jeweils die gleiche Anzahl an Trainings- und Testdaten vorhanden sein. Aus diesem Grund werden für das Training aus jeder Klasse nur die Anzahl der Szenarien verwendet, die mindestens in jeder Klasse verfügbar sind. Damit dieser Ansatz auch in der Praxis verwendet werden kann wird außerdem darauf geachtet, dass 95\% synthetische Daten und 5\% reale Daten für das Training verwendet werden. Mit dieser Verteilung bleibt der manuelle Aufwand für das Labeln der Trainingsdaten überschaubar. Die daraus resultierende Aufteilung von synthetischen und realen Szenarien auf Trainings-, Validierungs- und Testdaten ist in Tabelle \ref{tab_daten_aufteilung} aufgeschlüsselt. Beim Training mit einzelnen Bildern wird die gleiche Verteilung angewendet und da jedes Szenario aus 15 Bilder besteht, können die Zahlen aus Tabelle \ref{tab_daten_aufteilung} einfach mit 15 multipliziert werden.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{l | c c c c c c | c}

& \multicolumn{3}{c}{\textbf{Synthetische Daten}} & \multicolumn{3}{c|}{\textbf{\textbf{Reale Daten}}} & \\
& Training & Validierung & Test & Training & Validierung & Test & \\
Szenario & 85\% & 10\% & 5\% & 65\% & 10\% & 25\% & Summe \\
\hline
free cruising & 807 & 95 & 48 & 43 & 7 & 17 & 1.017 \\
following & 807 & 95 & 48 & 43 & 7 & 17 & 1.017 \\
catching up & 807 & 95 & 48 & 43 & 7 & 17 & 1.017 \\
lane change left & 807 & 95 & 48 & 43 & 7 & 17 & 1.017 \\
lane change right & 807 & 95 & 48 & 43 & 7 & 17 & 1.017 \\
\hline
\textbf{Summe} & \textbf{4.035} & \textbf{475} & \textbf{240} & \textbf{215} & \textbf{35} & \textbf{85} & 5.085 \\
\hline

\end{tabular}
\caption{Aufteilung von synthetischen und realen Szenarios auf Trainings-, Validierungs- und Testdaten}
\label{tab_daten_aufteilung}
\end{table}

Die Bilddaten werden für das Training außerdem in die Form 299 x 299 x 3 Pixel transformiert. Diese Bildgröße ist ein Kompromiss zwischen Detailgrad und Trainingszeit und wird in Keras, neben der Auflösung 244 x 244 x 3 Pixel, für \acp{CNN} als Standardkonfiguration vorgeschlagen \cite{chollet2015keras}.

% ===========================
\subsection{Architektur der künstlichen neuronalen Netze}
\label{umsetzung_training_architektur}
% ===========================

Wie in Abschnitt \ref{grundlagen_nn_video} erläutert, gibt es verschiedenen Ansätze um Bildsequenzen mit \acp{KNN} zu klassifizieren. In dieser Arbeit werden zwei unterschiedliche Ansätze umgesetzt und miteinander verglichen. Im ersten Ansatz werden die Bilder aus den Bildersequenzen einzeln klassifiziert. Anschließend wird den Sequenzen die Klasse zugeordnet, mit der die meisten Bilder aus der jeweiligen Sequenz klassifiziert wurden (Mehrheitsprinzip). Für den zweiten Ansatz wird eine Kombination aus Bild- und Sequenzerkennung verwendet um die Bildersequenzen als Ganzes zu klassifizieren. Die Architekturken für den jeweiligen Ansatz sind schematisch in Abbildung \ref{fig_ansaetze_architekturen} dargestellt und werden im Detail in den folgenden Absätzen erklärt. Dazu werden zu Beginn die Architekturen und Konfigurationen der verwendeten \acp{CNN} vorgestellt. Anschließend werden die verwendeten Fully-Connected-, Dropout- und \ac{LSTM}-Schichten mit den verwendeten Parametern vorgestellt. Am Ende dieses Abschnitts werden alle Komponenten zusammengefügt und acht Architekturen vorgestellt, die für die Experiment in Abschnitt \ref{umsetzung_training_experimente} verwendet werden.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Bilderkennung mit anschließendem Mehrheitsprinzip]{\includegraphics[scale=0.6]{ansatz_bilderkennung.pdf}} \\
\subfloat[Kombination aus Bild- und Sequenzerkennung]{\includegraphics[scale=0.6]{ansatz_bildsequenzerkennung.pdf}}
\end{tabular}
\caption{Zwei Ansätze für die Videoklassifizierung}
\label{fig_ansaetze_architekturen}
\end{figure}

% ===========================
\subsubsection{\aclp{CNN}}
% ===========================

Das Training mit \acp{CNN} hat in den vergangenen Jahre große Fortschritte gemacht durch die Verfügbarkeit von großen Datenmengen, steigender Rechenleistung und die Wiederverwendung von bereits vortrainierten \acp{CNN}. Diese Verwendung von bereits trainierten Netzen nennt sich Transferlernen (engl. transfer learning) \cite{oquab2014transfer}. Die Idee von einem \ac{CNN} ist es in den frühen Schichten sehr grundlegende Merkmale (engl. low-level features) und in späteren Schichten abstraktere Merkmale (engl. high-level features) zu extrahieren. Schließlich wird Bilder auf Basis dieser abstrakten Merkmale mithilfe von einer oder mehreren Fully-Connected-Schichten klassifiziert. Diese Funktionsweise macht man sich beim Transferlernen zunutze und verwendet ein \ac{CNN} das bereits auf vielen Millionen Bildern trainiert wurde. Dieses \ac{CNN} hat bereits gelernt unterschiedliche Merkmale aus Bildern zu extrahieren. Um ein ein solches neuronales Netz für die eigene Arbeit verwenden zu können, muss man lediglich die letzten Fully-Connected-Schichten ersetzen und mit den eigenen Bilddaten und entsprechenden Klassen trainieren \cite{oquab2014transfer}. Mit diesem Verfahren kann viel Zeit und Rechenleistung gespart werden, weil die grundlegende Merkmalsextraktion nicht grundlegend neu gelernt werden muss. 

Das Transferlernen wird auch in dieser Arbeit verwendet. Dazu wurden verschiedene \ac{CNN}-Architekturen verglichen und schließlich zwei unterschiedliche für diese Arbeit ausgewählt. Eine Übersicht der bekanntesten vortrainierten \acp{CNN} ist in Abbildung \ref{fig_comparison_cnns} dargestellt. In dieser Darstellung werden die neuronalen Netze anhand ihrer erreichen Genauigkeit (engl. accuracy) und der benötigten Operationen bzw. ihrer Parameter bei der Berechnung einer Klassifizierung eingeordnet \cite{canziani2016analysis}.

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{comparison_cnns.png}
\caption{Vergleich von vortrainierten \aclp{CNN}, entnommen aus \cite{canziani2016analysis}}
\label{fig_comparison_cnns}
\end{figure}

Für die Architekturen in dieser Arbeit werden die \acp{CNN} Inception-V3 \cite{szegedy2016inception} und VGG-16 \cite{simonyan2014vgg} als Basis verwendet, weil beide in vergangene Wettbewerben und anderen Arbeiten sehr gute Ergebnisse erzielen konnten.

Das VGG-16 Netz \cite{simonyan2014vgg} ist ein tiefes \acl{CNN}, das aus mehreren Convolution-, Pooling und Fully-Connected-Schichten besteht. Dabei sind die Schichten, wie in Abschnitt \ref{grundlagen_nn_cnn} beschrieben, aufeinander gestapelt. Das Netz umfasst insgesamt 138.357.544 trainierbare Parameter, die in dieser Arbeit auf Basis des ImageNet Datensatz vortrainiert waren. Die Architektur des Netztes ist in Abbildung \ref{fig_vgg_16} dargestellt. Davon werden in dieser Arbeit alle Convolution- und Pooling-Schichten verwendet. Die Fully-Connected- und Softmax-Schichten am Ende des Netzes werden entfernt, da sie auf die Klassifizierung der der ImageNet-Klassen angepasst sind. An ihrer Stelle werden andere Schichten eingefügt, die in weiteren Verlauf dieses Abschnitts erklärt werden.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{vgg16.png}
\caption{Architektur des VGG-16 Netzes, entnommen aus \cite{cord2018vgg16}}
\label{fig_vgg_16}
\end{figure}

ImageNet ist ein Datensatz mit 14.197.122 Bildern und 21.841 sogenannten Synonymmengen (engl. synonym sets) \cite{deng2009imagenet}. In Synonymmengen sind Wörter der gleichen Bedeutung zusammengefasst. Dabei sind alle Wörter auch hierarchisch nach dem WordNet-Projekt eingeordnet.

Im Gegensatz zu einfach gestapelten Convolution- und Pooling-Schichten wie beispielsweise dem VGG-16 Netz, orientiert sich das Inception-V3 Netz an zusätzlichen Prinzipen, um die Genauigkeit zu verbessern und die Rechenleistung zu senken. Zusätzlich werden Convolution-Operationen nicht nur gestapelt, sondern auch parallel zu anderen Convolution-Operationen berechnet, was zu einer besseren Leistung des Netzes führt \cite{szegedy2016inception}. 

Es wird argumentiert, dass ein Filter mit der Dimension 5x5 durch zwei aufeinanderfolgende Filter der Dimension 3x3 ersetzt werden kann, was die Rechenleistung deutlich reduziert \cite{szegedy2016inception}. Diese Funktionalität wird in einem sogenannten Inception-Modul umgesetzt, was in Abbildung \ref{fig_inc_fig} unter (a) zu sehen ist. Ein weiteres Prinzip ist, dass ein nxn-Filter mit einen 1xn-Filter gefolgt von einem nx1 Filter ersetzt werden kann, bei gleichbleibender Leistung und 33\% weniger Rechenaufwand. Dieses Prinzip ist in Abbildung \ref{fig_inc_fig} unter (b) dargestellt. Für das dritte Prinzip wird argumentiert, dass bei einer starken Dimensionsreduzierung durch Pooling-Schichten viel Information aus dem Bild verloren gehen kann. Um dies zu verhindern, werden statt einer Pooling-Schicht mehrere parallele Convolution-Operationen ausgeführt. Dieses Prinzip wird mit dem Inception-Modul aus Abbildung \ref{fig_inc_fig} unter (c) umgesetzt.

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
\subfloat[Inception-Modul in dem ein 5x5-Filter mit zwei 3x3-Filter ersetzt wird]{\includegraphics[scale=0.3]{inc_fig_5.pdf}} &
\subfloat[Inception-Modul in dem ein 7x7-Filter mit einen 1x7-Filter gefolgt von einem 7x1 Filter ersetzt wird]{\includegraphics[scale=0.3]{inc_fig_6.pdf}} &
\subfloat[Inception-Modul in statt einer Pooling-Schicht mehrere parallele Convolution-Operationen ausgeführt werden]{\includegraphics[scale=0.3]{inc_fig_7.pdf}}
\end{tabular}
\caption{Inception-Module aus der Inception-V3 Architektur \cite{szegedy2016inception}}
\label{fig_inc_fig}
\end{figure}

Die Architektur des gesamten Netzes ist in Abbildung \ref{fig_inception_v3} dargestellt. In dieser Arbeit wird ein Inception-V3 Netz verwendet, das mit dem Datenset ImageNet \cite{deng2009imagenet} trainiert wurde. Wie bei dem VGG-16 Netz werden in dieser Arbeit die letzten Schichten (in der Abbildung der \textit{final part}) am Ende des Netzes entfernt und ebenfalls andere Schichten eingefügt, die in den nachfolgenden Absätzen erklärt werden.

\begin{figure}[h]
\centering
\includegraphics[scale=0.45]{inception_v3.png}
\caption{Architektur des Inception-V3 Netzes, entnommen aus \cite{google2018inceptionv3}}
\label{fig_inception_v3}
\end{figure}

% ===========================
\subsubsection{Fully-Connected-, Dropout- und \ac{LSTM}-Schicht}
% ===========================

Eine Fully-Connected-Schicht ist, wie in Abschnitt \ref{grundlagen_nn_entwicklung} beschrieben, eine normale Schicht mit $n$ Neuronen in einem Multilayer-Perzeptron. Diese Schicht muss mit einer Anzahl  Neuronen $n$ und einer Aktivierungsfunktion $\varphi$ konfiguriert werden. Die Ergebnisschicht in einem \ac{KNN} ist ebenfalls immer eine Fully-Connected-Schicht. In dieser Arbeit werden in jeder Architektur zwei dieser Schichten verwendet, eine nach dem vortrainierten \ac{CNN} bzw. der \ac{LSTM}-Schicht und die andere als Ergebnisschicht. Die erste Fully-Connected Schicht besteht in dieser Arbeit immer aus 128 Neuronen und der \ac{ReLU}-Aktivierungsfunktion \cite{nair2010rectified}. In der Ergebnisschicht werden alle zuvor extrahierten Merkmale auf die Anzahl der Klassen heruntergebrochen und daher besteht die Ergebnisschicht aus der gleichen Anzahl Neuronen wie es Klassen gibt. Als Aktivierungsfunktion für ein Multiklassen-Problem ($C>2$), wie in dieser Arbeit, wird Softmax als Aktivierungsfunktion verwendet \cite{bishop2006pattern}.

\todo{Exkurs Regularisierung}

Dropout ist eine Regularisierungs-Methode um beim Training von \acp{KNN} Überanpassung (engl. Overfitting) zu vermeiden und insgesamt bessere Leistung bei der Kombination von verschiedenen Architekturen zu erreichen \cite{srivastava2014dropout}. Die Grundidee von Dropout ist es die Komplexität eines \acp{KNN} zu reduzieren, indem bei jedem Trainingsschritt zufällig einige Neuronen und die zugehörigen Kanten im Netz, die Gewichte, entfernt. Dafür wird einer Schicht im Netz eine Rate $r$ zugeordnet. Diese Rate $r$ gibt an, welcher Anteil der Neuronen aus dieser Schicht zufällig entfernt wird. Bei $r=0,5$ werden beispielsweise 50\% aller Neuronen in jedem Trainingsschritt zufällig entfernt. Das Ergebnis ist, dass das \ac{KNN} nicht zu abhängig von einzelnen Neuronen oder Gewichten wird und insgesamt bessere Ergebnisse erzielt werden können \cite{srivastava2014dropout}. In dieser Arbeit wird Dropout mit der Rate $r=0,5$ verwendet. Obwohl Dropout keine eigenständige Schicht ist, wird es in Keras als Schicht modelliert. Dabei bezieht sich Dropout aber stets auf die jeweilige Schicht davor. Ein Beispiel von Dropout ist in Abbildung \ref{fig_dropout} dargestellt.

\begin{figure}[h]
\centering
\begin{tabular}{c@{\hskip 1.5cm}c}
\subfloat[\ac{KNN} ohne Dropout]{\includegraphics[scale=1.5]{dropout_1.pdf}} &
\subfloat[\ac{KNN} mit Dropout in der mittleren Schicht mit einer Wahrscheinlichkeit von $r=0,5$]{\includegraphics[scale=1.5]{dropout_2.pdf}}
\end{tabular}
\caption{Regularisierung mit Dropout \cite{srivastava2014dropout}}
\label{fig_dropout}
\end{figure}

Wie in Abschnitt \ref{grundlagen_nn_rnn} beschrieben, werden \ac{LSTM}-Schichten für die Sequenzerkennung eingesetzt. Da die \ac{LSTM}-Schicht in dieser Arbeit nach der Merkmalsextraktion mit einem \ac{CNN} verwendet wird, ist in Keras eine sogenannte \textit{TimeDistributed}-Schicht nötig, um die Merkmale aller Bilder einer Sequenz der \ac{LSTM}-Schicht übergeben zu können. Diese Umhüllung (engl. wrapper) hat keine Parameter und keinen Einfluss auf die Klassifizierung. Die eigentliche \ac{LSTM}-Schicht wird mit der Ausgangsdimension $units = 256$ und der \ac{tanh}-Aktivierungsfunktion konfiguriert.

% ===========================
\subsubsection{Komponenten zusammenfügen}
% ===========================

Auf Basis der oben beschriebenen Komponenten werden jetzt acht verschiedene \ac{KNN}-Architekturen designed. Dabei wird zwischen den Architekturen für die Klassifizierung einzelner Bilder und den Architekturen für die Klassifizierung von Bildersequenzen unterschieden. Beide grundlegenden Architekturen sind schematisch in Abbildung \ref{fig_architekturen_grundlegend} dargestellt.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Architektur für die Klassifizierung einzelner Bilder]{\includegraphics[scale=0.6]{architektur_1.pdf}} \\
\subfloat[Architektur für die Klassifizierung von Bildersequenzen]{\includegraphics[scale=0.658]{architektur_2.pdf}}
\end{tabular}
\caption{Grundlegende \ac{KNN}-Architekturen in dieser Arbeit}
\label{fig_architekturen_grundlegend}
\end{figure}

Ausgehend von der Darstellung in Abbildung \ref{fig_architekturen_grundlegend} wird bei den Architekturen für die Klassifizierung von einzelnen Bildern zwischen den zwei \acp{CNN} und zwischen einer Version mit und ohne Dropout unterschieden. Damit ergeben sich vier verschiedene Architekturen, die in Tabelle \ref{tab_architekturen_bild} mit den jeweiligen Parametern zusammengefasst sind.

\begin{table}[h]
\centering
\small
\def\arraystretch{1.4}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Architekturen}} \\
\hline
\textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\
\hline
\hline
\multicolumn{4}{|c|}{Input shape: 299x299x3} \\
\hline
\multicolumn{2}{|c|}{Inception-V3} & \multicolumn{2}{|c|}{VGG-16} \\
\multicolumn{2}{|c|}{Output shape: 2048} & \multicolumn{2}{|c|}{Output shape: 512} \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht} \\
\multicolumn{4}{|c|}{$units=128$, $activation=\text{\ac{ReLU}}$} \\
\multicolumn{4}{|c|}{Output shape: 128} \\
\hline
ohne Dropout & Dropout & ohne Dropout & Dropout \\
- & $r=0,5$ & - & $r=0,5$ \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht (Ergebnisschicht)} \\
\multicolumn{4}{|c|}{$units=5$, $activation=Softmax$} \\
\multicolumn{4}{|c|}{Output shape: 5} \\
\hline
\end{tabular}
\caption{\ac{KNN}-Architekturen für die Klassifizierung von Bildersequenzen}
\label{tab_architekturen_bild}
\end{table}

Analog zu den Architekturen für die Klassifizierung von einzelnen Bildern, werden bei den Architekturen für die Klassifizierung von Bildersequenzen ebenfalls zwei verschiedene \ac{CNN}-Architekturen variiert und mit und ohne Dropout gearbeitet. Die resultierenden vier Architekturen sind in Tabelle \ref{tab_architekturen_video} mit ihren Parametern zusammengefasst.

\begin{table}[h]
\centering
\small
\def\arraystretch{1.4}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Architekturen}} \\
\hline
\textbf{E} & \textbf{F} & \textbf{G} & \textbf{H} \\
\hline
\hline
\multicolumn{4}{|c|}{Input shape: 15x299x299x3} \\
\hline
\multicolumn{2}{|c|}{\textit{TimeDistributed} mit Inception-V3} & \multicolumn{2}{|c|}{\textit{TimeDistributed} mit VGG-16} \\
\multicolumn{2}{|c|}{Output shape: 15x2048} & \multicolumn{2}{|c|}{Output shape: 15x512} \\
\hline
\multicolumn{4}{|c|}{\ac{LSTM}-Schicht} \\
\multicolumn{4}{|c|}{$units=256$, $activation=\text{\ac{tanh}}$} \\
\multicolumn{4}{|c|}{Output shape: 256} \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht} \\
\multicolumn{4}{|c|}{$units=128$, $activation=\text{\ac{ReLU}}$} \\
\multicolumn{4}{|c|}{Output shape: 128} \\
\hline
ohne Dropout & Dropout & ohne Dropout & Dropout \\
- & $r=0,5$ & - & $r=0,5$ \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht (Ergebnisschicht)} \\
\multicolumn{4}{|c|}{$units=5$, $activation=Softmax$} \\
\multicolumn{4}{|c|}{Output shape: 5} \\
\hline
\end{tabular}
\caption{\ac{KNN}-Architekturen für die Klassifizierung einzelner Bilder}
\label{tab_architekturen_video}
\end{table}


% ===========================
\subsection{Wahl der Parameter}
\label{umsetzung_training_experimente}
% ===========================

Bei der Konfiguration von \acp{KNN} und den einzelnen Schichten können viele unterschiedliche Parameter konfiguriert werden. Da es sich bei dieser Arbeit um einen Proof-of-Concept handelt, wurden sowohl im letzten Abschnitt \ref{umsetzung_training_architektur} bei der Konfiguration der \ac{KNN}-Architekturen, als auch in diesem Abschnitt, oftmals auf die Standardkonfiguration von Keras zurückgegriffen. Diese Standardkonfiguration basiert nach Chollet stets auf aktuellen Forschungsergebnissen und dem Stand der Technik bei \acp{KNN} \cite{chollet2015keras}. Diese Vorgehensweise erlaubt einerseits eine schnelle Umsetzung des Proof-of-Concept dieser Arbeit und bietet andererseits in nachfolgenden Arbeiten viel Potential die Ergebnisse weiter zu verbessern.

Vor jedem Training werden die \acp{KNN} mit einem Optimierer (engl. optimizer) und einer Fehlerfunktion (engl. loss function) kompiliert. Als Optimierer wird in dieser Arbeit Adam (adaptive moment estimation) verwendet. Dieser Optimierer vereint die Vorteile von den Optimierern AdaGrad (adaptive gradient algorithm) und RMSProp (root mean square propagation). Das Grundprinzip ist es, die Lernrate dynamisch aanzupassen, was zu weniger benötigtem Speicher und einer hohen Konvergenzrate führt \cite{kingma2014adam}.

 Neben der mittleren quadratischen Abweichung, die in der Abschnitt \ref{grundlagen_nn_entwicklung} vorgestellt wurde, gibt es weitere Fehlerfunktionen. In dieser Arbeit wird die Kreuzentropie (engl. cross entropy) für Mehrklassenprobleme mit der folgenden Fehlerfunktion verwendet \cite{bishop2006pattern}:

\begin{equation}
E(w_1, ..., w_K) = -\sum _{n=1}^N \sum _{k=1}^K t_{nk} * ln(y_{nk})
\end{equation}

Dabei beschreiben $w_1, ..., w_K$ die zu optimierenden Gewichte, $N$ ist die Anzahl der Trainingsdaten und $K$ die Anzahl der Klassen. $t_{nk}$ gibt mit dem Wert $1$ an, ob der Dateninput $n$ zur Klasse $k$ gehört. Wenn $n$ nicht zur Klasse $k$ gehört, nimmt $t_{nk}$ den Wert $0$ an. $y_{nk}$ ist die berechnete Wahrscheinlichkeit des Klassifikators, dass der Dateninput $n$ zur Klasse $k$ gehört.

Das Ziel der Experimente ist eine möglichst hohe Genauigkeit (engl. accuracy) bei der Klassifizierung von Bildsequenzen. Bei Mehrklassenproblemen (engl. multi-class problem) werden häufig die Top-1 und Top-5 Genauigkeit bzw. der Top-1 und Top-5 Fehler untersucht \cite{szegedy2016inception, simonyan2014vgg}. Das Ergebnis einer Multiklassen-Klassifikation für jeden Input ist ein Vektor mit Wahrscheinlichkeiten für jede Klasse. Bei der Top-1 Genauigkeit bzw. dem Top-1 Fehler wird jeweils nur die Klasse mit der höchsten Genauigkeit betrachtet und mit der richtigen Klasse abgeglichen. Bei der Top-5 Genauigkeit bzw. dem Top-5 Fehler werden jeweils die fünf Klassen mit den höchsten Wahrscheinlichkeit mit der richtigen Klasse abgeglichen. Da in dieser Arbeit nur insgesamt fünf Klassen betrachtet werden, wird nur die Top-1 Genauigkeit als Metrik verwendet.

Anhand dieser Metrik werden die unterschiedlichen Architekturen aus dem vorherigen Abschnitt \ref{umsetzung_training_architektur} mit jeweils 10 und 100 Epochen trainiert und dann getestet. Aus der Variation von vier Parametern mit jeweils zwei Ausprägungen ergeben sich insgesamt 16 Experimente. Die Parameter mit ihren Ausprägungen sind in Tabelle \ref{tab_parameter} zusammengefasst.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{l l}
\textbf{Parameter} & \textbf{Ausprägungen} \\
\hline
Prinzip der Klassifizierung & einzelne Bilder, Bildersequenzen \\
\acl{CNN} & Inception-V3, VGG-16 \\
Regularisierung & ohne Dropout, mit Dropout \\
Epochen & 10, 100 \\
\hline
\end{tabular}
\caption{Parameter die in den Experimenten variiert werden}
\label{tab_parameter}
\end{table}













 


 
 

