
% ===========================
\chapter{Implementierung}
\label{umsetzung}
% ===========================

In diesem Kapitel wird die Umsetzung des Konzepts aus dem vorherigen Kapitel beschrieben. Zu Beginn werden die dafür notwendigen Fahrszenarien in Abschnitt \ref{umsetzung_definition} definiert. Danach wird in den Abschnitten \ref{umsetzung_daten_synth} und \ref{umsetzung_daten_real} die Methodik für die Generierung von synthetischen und realen Daten erläutert. Im Anschluss wird in Abschnitt \ref{umsetzung_training} die Architektur und die Experimente der Klassifikatoren für die Szenarienerkennung beschrieben.


% ===========================
\section{Definition der Fahrszenarien}
\label{umsetzung_definition}
% ===========================

In diesem Abschnitt werden Szenarien, wie in Abschnitt \ref{grundlagen_fahren_szenarien} beschrieben, als \textit{logische Szenarien} für das weitere Vorgehen in dieser Arbeit definiert. In Anlehnung an bestehende Arbeiten zur Erkennung von Fahrszenarien und auf Basis von Machbarkeitsabschätzungen für die Umsetzung werden in dieser Arbeit die Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right} auf der Autobahn betrachtet. Die Autobahn wurde ausgewählt, weil es weniger Details zu betrachten gibt als auf anderen Straßen wie beispielsweise in der Stadt. In der folgenden Tabelle \ref{tab_definition_szenarios} werden diese Szenarien auf \textit{funktionaler} und \textit{logischer Ebene} definiert.

Um die Darstellung in der Tabelle zu erleichtern, werden folgende Abstände zwischen Ego-Fahrzeug und Fahrzeug 2 definiert. Dabei beschreibt $ego_v$ die Geschwindigkeit des Ego-Fahrzeugs in [m/s].

\begin{equation*}
\begin{split}
s_0 = ego_v * 3,6 \qquad \text{[m]} \\
s_1 = ego_v * 3,6 * \frac{2}{3} \qquad \text{[m]} \\
s_2 = ego_v * 3,6 * \frac{1}{2} \qquad \text{[m]} \\
s_3 = ego_v * 3,6 * \frac{1}{3} \qquad \text{[m]} \\
\end{split}
\end{equation*}

\begin{longtable}[c]{p{2.5cm} p{5.5cm} p{5.5cm}}
\textbf{Szenario} & \textbf{Funktionale Definition} & \textbf{Logische Definition} \\
\hline
\endhead

\textbf{Alle} & 2-spurige Autobahn geradeaus oder in einer Kurve, Geschwindigkeitsbegrenzung ist größer als 80 km/h & Breite Fahrstreifen [2,3..3,5] m \newline Geschwindigkeitsbegrenzung [80..keine] km/h \\
\hline

\textbf{Alle} & Tageslicht, keine Wolken bis leicht bewölkt, kein Niederschlag, gute Sichtbedingungen & Tageszeit [Sonnenaufgang..Sonnenuntergang] \newline Bewölkung [leicht bewölkt..wolkenlos]\\
\hline \hline

\textbf{Free cruising} & Ego, keine anderen Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt frei auf linker oder rechter Fahrspur, andere Fahrzeuge sind weit entfernt und haben keinen Einfluss auf die Manöver des Ego & Geschwindigkeit Ego [60..200] km/h \newline Abstand zu anderen Verkehrsteilnehmern [$>s_0$] m \\
\hline

\textbf{Approaching} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego nähert sich auf linker oder rechter Fahrspur in mittlerem Abstand dem Fahrzeug 2 & Geschwindigkeit Ego abnehmend [60..200] km/h \newline Geschwindigkeit Ego < Geschwindigkeit Fahrzeug 2 \newline Abstand Ego zu Fahrzeug 2 [$s_2$..$s_0$] m \\
\hline

\textbf{Following} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt auf linker oder rechter Fahrspur in sicherem Abstand hinter Fahrzeug 2 & Geschwindigkeit Ego [60..200] km/h \newline Geschwindigkeitsdifferenz zwischen Ego und Fahrzeug 2 < Geschwindigkeit Ego * 0,05 km/h \newline Abstand Ego zu Fahrzeug 2 [$s_3$..$s_1$] m \newline Ego befindet sich auf gleicher Fahrspur hinter Fahrzeug 2 \\
\hline

\textbf{Catching up} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt auf der linken Fahrspur und verringert den vertikalen Abstand zu Fahrzeug 2, das sich vor dem Ego-Fahrzeug auf der rechten Fahrspur befindet & Geschwindigkeit Ego [60..200] km/h \newline Geschwindigkeit Fahrzeug 2 < Geschwindigkeit Ego \newline Vertikaler Abstand Ego zu Fahrzeug 2 [0..$s_0$] m \newline Ego fährt auf linker Fahrspur hinter Fahrzeug 2 das auf rechter Fahrspur fährt \\
\hline

\textbf{Overtaking} & Ego, andere Verkehrsteilnehmer \newline \underline{Interaktion:} Ego fährt auf der linken Fahrspur und vergrößert den vertikalen Abstand zu Fahrzeug 2, das sich hinter dem Ego-Fahrzeug auf der rechten Fahrspur befindet & Geschwindigkeit Ego [60..200] km/h \newline Geschwindigkeit Fahrzeug 2 < Geschwindigkeit Ego \newline Vertikaler Abstand Ego zu Fahrzeug 2 [0..$s_0$] m \newline Ego fährt auf linker Fahrspur vor Fahrzeug 2 das auf rechter Fahrspur fährt \\
\hline

\textbf{Lane change left} & Ego, andere Verkehrsteilnehmer sind optional \newline \underline{Interaktion:} Ego fährt auf rechter Fahrspur und wechselt auf linke Fahrspur & Geschwindigkeit Ego [60..200] km/h \newline Ego befindet sich auf rechter Fahrspur und wechselt auf linke Fahrspur \\
\hline

\textbf{Lane change right} & Ego, andere Verkehrsteilnehmer sind optional \newline \underline{Interaktion:} Ego fährt auf linker Fahrspur und wechselt auf rechte Fahrspur & Geschwindigkeit Ego [60..200] km/h \newline Ego befindet sich auf linker Fahrspur und wechselt auf rechte Fahrspur \\
\hline

\caption{Definitionen der Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right}}
\label{tab_definition_szenarios}
\end{longtable}

% ===========================
\section{Generierung synthetischer Daten}
\label{umsetzung_daten_synth}
% ===========================

Auf Basis der Definitionen aus dem vorherigen Abschnitt \ref{umsetzung_definition} werden in diesem Abschnitt die benötigten Signal- und Bilddaten simuliert und entsprechend annotiert. Dafür werden in Abschnitt \ref{umsetzung_daten_synth_simulation} die Signaldaten, die für die eindeutige Klassifizierung der Szenarien benötigt werden, simuliert. In Abschnitt \ref{umsetzung_daten_synth_labeling} werden diese Signaldaten verwendet, um die parallel simulierten Bilddaten entsprechend zu annotieren.
 
% ===========================
\subsection{Simulation}
\label{umsetzung_daten_synth_simulation}
% ===========================

Für die Simulation der Signal- und Bilddaten wird die kommerzielle Software CarMaker von IPG Automotive \cite{ipg2018carmaker} verwendet. Diese Simulationssoftware wird für den virtuellen Fahrversuch und \ac{HiL}-Tests eingesetzt, um Komponenten in unterschiedlichen Szenarien zu testen. In dieser Arbeit wird CarMaker verwendet, um die Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right} zu simulieren.

Für die Aufnahme der benötigten Bilddaten wird im simulierten Fahrzeug ein entsprechender Kamerasensor konfiguriert. Die Konfiguration des Sensors orientiert sich an der Konfiguration von realen Frontview-Kameras im Fahrzeug nach Punke \cite{punke2015kamera}. So ist der Kamerasensor an der Stelle des Rückfahrspiegels platziert, hat eine Auflösung von 640 x 480 Pixel und ein Sichtfeld von 20°.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{p{7cm} p{6.8cm}}
\textbf{Variable in CarMaker} & \textbf{Beschreibung} \\
\hline

Car.v & Geschwindigkeit des Ego-Fahrzeugs in [m/s] \\
Car.Road.sRoad & Position des Ego-Fahrzeugs auf der Strecke in [m] \\
Car.Road.Lane.Act.LaneId & Fahrspur-ID des Ego-Fahrzeugs \\
\hline
Sensor.Object.OB01.TX.NearPnt.dv\_p & Geschwindigkeitsdifferenz zwischen Fahrzeug TX und dem Ego-Fahrzeug in [m/s] \\
Sensor.Object.OB01.TX.NearPnt.ds\_p & Abstand zwischen Fahrzeug TX und dem Ego-Fahrzeug in [m] \\
\hline
Traffic.TX.sRoad & Position des Fahrzeugs TX auf der Strecke in [m] \\
Traffic.TX.Lane.Act.LaneId & Fahrspur-ID des Fahrzeugs TX \\
\hline

\end{tabular}
\caption{Aufgezeichnete Signaldaten in CarMaker}
\label{tab_output_quantities}
\end{table}

Die benötigten Signaldaten für das Annotieren werden von den Definitionen aus Abschnitt \ref{umsetzung_definition} abgeleitet. Für die eindeutige Identifikation der \textit{logischen Szenarien} werden die folgenden Werte benötigt: Geschwindigkeit des Ego-Fahrzeugs, Abstand und Geschwindigkeitsdifferenz des Ego-Fahrzeugs zu allen anderen Fahrzeugen, aktuelle Fahrspur des Ego-Fahrzeugs und allen anderen Fahrzeugen und die relative Position des Ego-Fahrzeugs, i.e. ob sich das Ego-Fahrzeug vor oder hinter einem anderen Fahrzeug befindet. Um den Abstand und die Geschwindigkeitsdifferenz des Ego-Fahrzeugs zu allen anderen Fahrzeugen aufzuzeichnen, wird ein Objektsensor im Ego-Fahrzeug konfiguriert. Mit diesem Sensor können im festgelegten Radius alle Fahrzeuge und ihr Abstand und ihre relative Geschwindigkeit zum Ego-Fahrzeug erfasst und über die Funktion \textit{OutputQuantities} in CarMaker aufgezeichnet werden. Die Geschwindigkeit des Ego-Fahrzeugs und die Fahrspur-ID und Position aller Fahrzeuge können direkt, ohne zusätzlichen Sensor, über die Funktion \textit{OutputQuantities} aufgezeichnet werden. Die jeweiligen Variablen in CarMaker sind in der Tabelle \ref{tab_output_quantities} zusammengefasst.

Für die Simulation werden zwei Strecken der Länge 6.000 m und 10.000 m mit dem \textit{CarMaker - Scenario Editor} erstellt. Bei beiden Strecken handelt es sich um eine 4-spurige Autobahn mit zwei Fahrspuren in jede Richtung. Die Fahrtrichtungen sind in der Mitte von einer Leitplanke getrennt und am Rand der Fahrbahn sind jeweils Standstreifen vorhanden. Abschnittsweise stehen neben der Fahrbahn einige Bäume, was in Abbildung \ref{fig_cm_road_strecke} mit grünen Streifen gekennzeichnet ist. Abbildung \ref{fig_cm_road_bild} zeigt die Konfiguration der simulierten Straße.

\begin{figure}[h]
\centering
\includegraphics[scale=0.25]{images/cm_road_bild.png}
\caption[Konfiguration der simulierten Straße]{Konfiguration der simulierten Straße \cite{ipg2018carmaker}}
\label{fig_cm_road_bild}
\end{figure}

Auf beiden Strecken wird autonomer, stochastisch verteilter Verkehr erzeugt, was CarMaker mit einer gesonderten Funktion unterstützt. Der Verkehr wird in einer niedrigen Dichte (10 Prozent) und einem 80-prozentigen Anteil Autos erzeugt. Andere Fahrzeuge sind Motorräder, Lastkraftwagen und Busse. In CarMaker ist eine Vielzahl an unterschiedlichen Fahrzeugen verfügbar, was wichtig ist, um möglichst viele unterschiedliche Szenarien zu generieren. Mit dieser Konfiguration werden auf der 10.000m-Strecke 131 Fahrzeuge und auf der 6.000m-Strecke 89 Fahrzeuge generiert.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Strecke 1]{\includegraphics[scale=0.8]{images/cm_road_1.png}} \\
\subfloat[Strecke 2]{\includegraphics[scale=0.8]{images/cm_road_2.png}}
\end{tabular}
\caption[Schema der simulierten Strecken 1 und 2]{Schema der simulierten Strecken 1 und 2 \cite{ipg2018carmaker}}
\label{fig_cm_road_strecke}
\end{figure}

Die Simulation und Generierung von Bild- und Signaldaten wird mit dem \textit{CarMaker - Test Manager} durchgeführt. Mit diesem Modul lassen sich Fahrten mit unterschiedlichen Konfigurationen simulieren. In dieser Arbeit werden die Variablen \textit{Geschwindigkeit}, \textit{Mindestabstand zu vorausfahrendem Fahrzeug}, \textit{Minimale Geschwindigkeitsdifferenz beim Überholen} und \textit{Aggressivität beim Überholen} auf beiden oben beschriebenen Strecken variiert. Die Werte der Variablen, die simuliert werden, sind in Tabelle \ref{tab_tm_variablen} aufgelistet und sind aus der Sicht des Ego-Fahrzeugs zu verstehen.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{p{8cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm} p{0.7cm}}
\textbf{Variable} & \textbf{Werte} & & & & \\
\hline
Geschwindigkeit in [km/h] & 100 & 120 & 140 & 160 & 180 \\
Mindestabstand zu vorausfahrendem Fahrzeug in [s] & 1,0 & 1,5 & 2,0 & & \\
Minimale Geschwindigkeitsdifferenz beim Überholen in [km/h] & 5 & 15 & 25 & & \\
Aggressivität beim Überholen & 0,2 & 0,6 & 1,0 & & \\
\hline
\end{tabular}
\caption{Variablen und Werte die in der Simulation verwendet werden}
\label{tab_tm_variablen}
\end{table}

Die ersten drei Variablen sind selbsterklärend und werden hier nicht weiter erläutert. Die Variable \textit{Aggressivität beim Überholen} (in CarMaker \textit{Overtaking Rate} ist eine Zahl zwischen 0 und 1. Dabei beschreibt 0 einen Fahrstil, bei dem sich der Fahrer sehr risikoavers beim Überholen verhält, i.e. Überholen nur in sehr sicheren Situationen. Je größer die Zahl wird, desto aggressiver wird der Überholvorgang und dementsprechend sinkt die Risikoaversion beim Überholen und der Fahrer überholt auch bei kritischen oder schlecht einsehbaren Situationen.

Mit diesen vier Variablen mit jeweils drei beziehungsweise fünf Werten ergeben sich 135 verschiedene Kombinationsmöglichkeiten. Somit werden auf beiden Strecken in Summe 270 Fahrten mit insgesamt 2.160 km simuliert. Signal- und Bilddaten werden mit einer Frequenz von 5 Hz aufgezeichnet, was in 326.108 aufgezeichneten Szenen resultiert. Diese Szenen werden im folgenden Abschnitt \ref{umsetzung_daten_synth_labeling} annotiert.


% ===========================
\subsection{Daten Annotation}
\label{umsetzung_daten_synth_labeling}
% ===========================

Für das Annotieren der Szenarien wird jede Szene auf Basis der Definition aus Abschnitt \ref{umsetzung_definition} mithilfe der Signaldaten klassifiziert. Die logischen Bedingungen für jedes Szenario sind dafür in Tabelle \ref{tab_szenarien_labeling} zusammengefasst. Auf Basis der CarMaker-Variablen aus Tabelle \ref{tab_tm_variablen} werden folgende Variablen definiert, um die nachfolgenden Bedingungen übersichtlicher darstellen zu können. Dabei beschreibt $v2$ jeweils das Fahrzeug, auf Basis dessen das jeweilige Szenario klassifiziert wird.

\begin{equation*}
\begin{split}
ego_v = \text{Car.v} \qquad \text{[m/s]} \\
ego_{sRoad} = \text{Car.Road.sRoad} \qquad \text{[m]} \\
ego_{laneID} = \text{Car.Road.Lane.Act.LanId} \qquad \text{[1, 2]} \\
v2_{dv} = \text{Sensor.Object.OB01.TX.NearPnt.dv\_p} \qquad \text{[m/s]} \\
v2_{ds} = \text{Sensor.Object.OB01.TX.NearPnt.dv\_s} \qquad \text{[m]} \\
v2_{sRoad} = \text{Traffic.TX.sRoad} \qquad \text{[m]} \\
v2_{laneID} = \text{Traffic.TX.Lane.Act.LaneId} \qquad \text{[1, 2]} \\
\end{split}
\end{equation*}

\begin{longtable}[c]{p{3.8cm} p{10cm}}
\textbf{Szenario} & \textbf{Bedingungen} \\
\hline
\endhead

\textbf{Free cruising} & $ego_v > 17$ \newline $s_0 < v2_{ds}$ \\
\hline
\textbf{Approaching} & $s_2 < v2_{ds} < s_0$ \newline $ego_{sRoad} < v2_{sRoad}$ \newline $ego_{laneID} = v2_{laneID}$ \newline $ego_v < \text{ Durchschnitt von } ego_v \text{ der letzten 3 Sekunden} $ \\
\hline
\textbf{Following} & $v2_{dv} < ego_v * 0,05$ \newline $s_3 < s_1$ \newline $ego_{sRoad} < v2_{sRoad}$ \newline $ego_{laneID} = v2_{laneID}$ \\
\hline
\textbf{Catching up} & $v2_{dv} < 0$ \newline $0 \leq v2_{ds} < s_0$ \newline $ego_{sRoad} \leq v2_{sRoad}$ \newline $ego_{laneID} = v2_{laneID} - 1$ \\
\hline
\textbf{Overtaking} & $0 \leq v2_{ds} < s_0$ \newline $v2_{sRoad} < ego_{sRoad}$ \newline $ego_{laneID} = v2_{laneID} - 1$ \\
\hline
\textbf{Lane change left} & $ego_{laneID}^{before} = ego_{laneID}^{after} + 1$ \newline Als Spurwechsel wird ein Intervall von 4 Sekunden betrachtet, in dessen Mitte die Variable ihren Wert wechseln muss\\
\hline
\textbf{Lane change right} & $ego_{laneID}^{before} = ego_{laneID}^{after} - 1$ \newline Als Spurwechsel wird ein Intervall von 4 Sekunden betrachtet, in dessen Mitte die Variable ihren Wert wechseln muss \\

\hline
\caption{Bedingungen für die Szenarien \textit{free cruising}, \textit{approaching}, \textit{following}, \textit{catching up}, \textit{overtaking}, \textit{lane change left} und \textit{lane change right}}
\label{tab_szenarien_labeling}
\end{longtable}

Im Anschluss an die Klassifizierung einzelner Zeitpunkte werden diese zu Szenarien zusammengefasst, wenn mindestens 15 Zeitpunkte (3 Sekunden) in Folge mit der gleichen Klasse annotiert wurden. Drei Sekunden wird aus den folgenden zwei Gründen als Länge für Szenarien in dieser Arbeit gewählt. Erstens orientiert sich diese Zeitspanne an den vorherigen Arbeiten zur Szenarienerkennung. Zweitens haben die zwei Szenarien \textit{lane change left} und \textit{lane change right} jeweils eine natürliche Länge von 3-4 Sekunden. Alle anderen Szenarien können länger sein, lassen sich aber in Blöcke von jeweils 3 Sekunden einteilen. Insgesamt werden 326.108 Zeitpunkte und 23.972 Szenarien klassifiziert. Die Anzahl der simulierten Szenarien nach Klasse ist in der Tabelle \ref{tab_verteilung_szenarien} dargestellt.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{l c}
\textbf{Szenario} & \textbf{Anzahl} \\
\hline
Free cruising & 2.545 \\
Approaching & 3.512 \\
Following & 3.601 \\
Catching up & 5.563 \\
Overtaking & 5.149 \\
Lane change left & 957 \\
Lane change right & 975 \\
Unknown & 1.670 \\
\hline
Summe & 23.972 \\
\hline
\end{tabular}
\caption{Anzahl der simulierten Szenarien nach Klasse}
\label{tab_verteilung_szenarien}
\end{table}

Es sind auch einige Szenarien als \textit{unknown} klassifiziert, da zwischen den definierten Szenarien andere Situationen auftreten können oder nicht die Mindestanzahl der konsekutiven Szenen erreicht wird. In manchen Zeitpunkten wird mehr als eine einzelne Szene annotiert. Beispielsweise kann sich das Ego-Fahrzeug gleichzeitig in der Szene \textit{catching up} und \textit{overtaking} befinden, wenn es auf der linken Fahrspur fährt und sich in vertikalem Abstand ein anderes Fahrzeug jeweils vor und hinter dem Ego-Fahrzeug befindet. Abbildung \ref{fig_beispiel_szenario_lcl} zeigt ein Beispiel des Szenarios \textit{lane change left} mit allen zugehörigen 15 Bildern.

\begin{figure}[h]
\centering
\begin{tabular}{c c c c c}
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl0.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl1.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl2.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl3.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl4.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl5.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl6.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl7.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl8.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl9.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl10.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl11.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl12.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl13.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcl_sim/lcl14.jpg}} \\
\end{tabular}
\caption[Beispiel eines simulierten Szenarios der Klasse \textit{lane change left}]{Beispiel eines simulierten Szenarios der Klasse \textit{lane change left} \cite{ipg2018carmaker}}
\label{fig_beispiel_szenario_lcl}
\end{figure}

% ===========================
\section{Generierung realer Daten}
\label{umsetzung_daten_real}
% ===========================

Für den Proof-of-Concept dieser Arbeit, die Erkennung von realen Fahrszenarien, werden neben den synthetischen Trainingsdaten auch reale Daten für das Training und die anschließenden Tests benötigt. Dafür werden im ersten Schritt bestehen Datensätze nach ihrer Nutzbarkeit untersucht.

Die bekanntesten Datensätze sind KITTI \cite{geiger2013vision}, BDD100K \cite{yu2018bdd100k}, Cityscapes \cite{cordts2016cityscapes} und Oxford RobotCar \cite{maddern20171}. Der Cityscapes und Oxford RobotCar Datensatz umfasst lediglich Bilder von Szenen in Städten und ist daher nicht nutzbar für diese Arbeit. Die Datensätze KITTI und BDD100K umfassen auch Videos von Autobahnfahrten, allerdings liegt der Fokus auf Objekterkennung in einzelnen Bildern oder semantischer Segmentation. Und es gibt jeweils nur sehr begrenzt Videos, die auf einer 2-spurigen Autobahn aufgenommen wurden. Daher können diese Datensätze in dieser Arbeit nicht verwendet werden. Als Alternative werden die Aufnahmen von zwei Fahrten auf der Autobahn und der Bundesstraße verwendet. Die Strecken sind in der Abbildung \ref{fig_strecken_real} abgebildet. 

\begin{figure}[h]
\centering
\begin{tabular}{c c}
\subfloat[Route 1: Mannheim - Rostock]{\includegraphics[scale=0.7]{images/route_1.png}} &
\subfloat[Route 2: Karlsruhe - Kandel]{\includegraphics[scale=0.7]{images/route_2.png}} \\
\end{tabular}
\caption[Routen für die Aufnahme der realen Bilddaten]{Routen für die Aufnahme der realen Bilddaten \cite{google2018route1, google2018route2}}
\label{fig_strecken_real}
\end{figure}

Die Aufnahme der Route 1 umfasst über sechs Stunden Videomaterial mit über einer Stunde Fahrt auf einer 2-spurigen Autobahn \cite{youtube2018video}. Davon werden manuell zwischen 50 und 180 Szenarien aus jeder Klasse annotiert. Da von den Szenarien \textit{lane change left} und \textit{lane change right} jeweils nur 50 Szenarien vorhanden sind, wird vom Autor eine Fahrt auf Route 2 mit einigen Spurwechseln aufgenommen. Das Ergebnis sind jeweils 17 weitere Szenarien in den Klassen \textit{lane change left} und \textit{lane change right}. Abbildung \ref{fig_beispiel_szenario_lcr_real} zeigt ein Beispiel des realen Szenarios \textit{lane change right} mit allen zugehörigen 15 Bildern.

\begin{figure}[h]
\centering
\begin{tabular}{c c c c c}
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame0.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame1.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame2.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame3.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame4.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame5.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame6.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame7.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame8.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame9.jpg}} \\
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame10.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame11.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame12.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame13.jpg}} &
\subfloat[]{\includegraphics[scale=0.1]{images/lcr_real/frame14.jpg}} \\
\end{tabular}
\caption{Beispiel eines realen Szenarios der Klasse \textit{lane change right}}
\label{fig_beispiel_szenario_lcr_real}
\end{figure}

In Tabelle \ref{tab_datensaetze} wird der gesamte Datensatz dieser Arbeit mit synthetischen und realen Daten, mit den synthetischen Datensätzen, die in Abschnitt \ref{grundlagen_nn_synthetisch} vorgestellt wurden, und realen Datensätzen, die in diesem Abschnitt vorgestellt werden, verglichen.

\begin{longtable}[c]{p{2cm} p{3.5cm} p{2.5cm} p{4cm}}
\textbf{Datensatz} & \textbf{Generierung} & \textbf{Umfang} & \textbf{Annotation} \\
\hline
\endhead

\cite{ros2016synthia} & synthetisch mit der Unity Development Platform & 200.000 Bilder & semantische Annotation auf Pixelebene von 11 Klassen  \\
\hline
\cite{johnson2017driving} & synthetisch mit dem Computerspiel GTA5 & 250.000 Bilder & Begrenzungsboxen für die Klasse Fahrzeuge \\
\hline
\cite{richter2016playing} & synthetisch mit dem Computerspiel GTA5 & 25.000 Bilder & semantische Annotation auf Pixelebene von 19 Klassen \\
\hline
\cite{tremblay2018training} & synthetisch mit 3D Modelle von Fahrzeugen und realen Szenen & 100.000 Bilder & Begrenzungsboxen für die Klasse Fahrzeuge \\
\hline
KITTI, \cite{geiger2013vision} & real mit Fahrten in der Region Karlsruhe & 22 Videos und 15.000 Bilder & Begrenzungsboxen mit 8 Klassen \\
\hline
BDD100K, \cite{yu2018bdd100k} & real mit Fahrten in der Städten New York, Berkeley, San Francisco und der Bay Area & 100.000 Videos (40 Sekunden) mit insgesamt 120 Mio. Bilder & für jeweils das zehnte Bild aus jedem Video: Fahrbahnbegrenzungslinien und Begrenzungsboxen für 10 Objekte; Für 10.000 Bilder semantische Annotation auf Pixelebene von 19 Klassen \\
\hline
Cityscapes, \cite{cordts2016cityscapes} & real mit Fahrten in 50 Städten in Deutschland & 25.000 Bilder & semantische Annotation auf Pixelebene von 30 Klassen \\
\hline
Oxford RobotCar, \cite{maddern20171} & real mit Fahrten in Oxford & knapp 20 Mio. Bilder & keine Annotation \\
\hline
Diese Arbeit & synthetisch mit CarMaker und real mit zwei Autobahnfahrten in Deutschland & 24.307 Szenarien (3 Sekunden) mit insgesamt 331.133 Bilder & 7 Klassen \\

\hline
\caption{Vergleich von synthetischen und realen Datensätzen}
\label{tab_datensaetze}
\end{longtable}


% ===========================
\section{Training}
\label{umsetzung_training}
% ===========================

In diesem Abschnitt wird zu Beginn das Format und der Import der Trainings- und Testdaten in das \ac{KNN} erklärt. Danach werden verschiedene Architekturen von \acp{KNN}, die in dieser Arbeit zum Einsatz kommen, vorgestellt und schließlich werden die durchgeführten Experimente mit diesen Architekturen erläutert. Die Vorbereitung der Daten und das Training wird mit Python und der Deep-Learning-Bibliothek Keras \cite{chollet2015keras} implementiert.


% ===========================
\subsection{Vorbereitung der Daten}
\label{umsetzung_training_input}
% ===========================

Die Generierung der synthetischen und realen Trainings-, Validierungs- und Testdaten wurde bereits in den vorherigen Abschnitten beschrieben. In diesem Abschnitt wird darauf eingegangen, wie diese Daten vorbereitet werden.

In dieser Arbeit werden \acp{CNN} für die Erkennung einzelner Bilder und Kombinationen aus \acp{CNN} und \acp{LSTM} für die Klassifizierung von Videos eingesetzt. Daher müssen sowohl einzelne Bilder, als auch Bildsequenzen in das jeweilige \ac{KNN} importiert werden. Da es sich um Datenmengen von insgesamt über 50 GB handelt, müssen die Daten mit einem Datenstrom (engl. data stream) eingespeist werden, da nicht alle Daten zu Beginn in den Arbeitsspeicher geladen werden können.

Die Deep-Learning-Bibliothek Keras besitzt bereits verschiedene Klassen, sogenannte \textit{DataGenerators}, für der Import von einzelnen Bildern und für sequenzielle Daten mit zwei Dimensionen (e.g. CSV-Dateien). Es gibt allerdings keine bereits nutzbare Lösung für den Import von sequentiellen Bildern, was 4-dimensionalen Daten entspricht (Anzahl Bilder, Höhe, Breite, Farbkanal). Aus diesem Grund wird die Lösung von Amidi \cite{amidi2017datagenerator} adaptiert um höher-dimensionale Datensätze importieren zu können.

Von den generierten synthetischen und realen Szenarien werden \textit{free cruising}, \textit{following}, \textit{catching up}, \textit{lane change left} und \textit{lane change right} für das Training ausgewählt. Das Szenario \textit{approaching} wird für den Proof-of-Concept in dieser Arbeit bewusst entfernt, weil es Ähnlichkeiten zu dem Szenario \textit{following} gibt. Diese Überschneidungen und Ähnlichkeiten zwischen Szenarien und ihren Einfluss auf das Training eines Klassifikators können in weiteren Arbeiten untersucht werden. Das Szenario \textit{overtaking} wird entfernt, weil es mit der konfigurierten Frontkamera nicht erfasst werden kann. In folgenden Arbeiten kann dieses Szenario mithilfe weiterer Kameraperspektiven oder anderen Signaldaten berücksichtigt werden.

Um das Ergebnis nicht zu verzerren, sollten für das Training mit \acp{KNN} in jeder Klasse jeweils die gleiche Anzahl an Trainings-, Validierungs- und Testdaten vorhanden sein. Aus diesem Grund werden für das Training aus jeder Klasse nur die Anzahl der Szenarien verwendet, die mindestens in jeder Klasse verfügbar sind. Damit dieser Ansatz auch in der Praxis verwendet werden kann wird außerdem darauf geachtet, dass 95 Prozent synthetische Daten und 5 Prozent reale Daten für das Training verwendet werden. Mit dieser Verteilung bleibt der manuelle Aufwand für die Annotation der Trainingsdaten überschaubar. Die daraus resultierende Aufteilung von synthetischen und realen Szenarien auf Trainings-, Validierungs- und Testdaten ist in Tabelle \ref{tab_daten_aufteilung} aufgeschlüsselt. Beim Training mit einzelnen Bildern wird die gleiche Verteilung angewendet und da jedes Szenario aus 15 Bildern besteht, können die Zahlen aus Tabelle \ref{tab_daten_aufteilung} dafür mit dem Faktor 15 multipliziert werden.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{l c c c c c c}

& \multicolumn{3}{c}{\textbf{Synthetische Daten}} & \multicolumn{3}{c}{\textbf{\textbf{Reale Daten}}} \\
& Training & Validierung & Test & Training & Validierung & Test \\
Szenario & 85\% & 10\% & 5\% & 65\% & 10\% & 25\% \\
\hline
free cruising & 665 & 190 & 95 & 33 & 17 & 17 \\
following & 665 & 190 & 95 & 33 & 17 & 17 \\
catching up & 665 & 190 & 95 & 33 & 17 & 17 \\
lane change left & 665 & 190 & 95 & 33 & 17 & 17 \\
lane change right & 665 & 190 & 95 & 33 & 17 & 17 \\
\hline
\textbf{Summe} & \textbf{3.325} & \textbf{950} & \textbf{475} & \textbf{165} & \textbf{85} & \textbf{85} \\
\hline

\end{tabular}
\caption{Aufteilung von synthetischen und realen Szenarios auf Trainings-, Validierungs- und Testdaten}
\label{tab_daten_aufteilung}
\end{table}

Die Bilddaten werden für das Training in die Form 299 x 299 x 3 Pixel transformiert. Diese Bildgröße ist ein Kompromiss zwischen Detailgrad und Trainingszeit und wird in Keras für die Inception-V3 und die Xception Architektur als Standardgröße vorgeschlagen \cite{chollet2015keras}.

% ===========================
\subsection{Design der \ac{KNN}-Architekturen}
\label{umsetzung_training_architektur}
% ===========================

Wie in Abschnitt \ref{grundlagen_nn_video} erläutert, gibt es verschiedenen Ansätze, um Bildsequenzen mit \acp{KNN} zu klassifizieren. In dieser Arbeit werden zwei unterschiedliche Ansätze umgesetzt und miteinander verglichen. Im ersten Ansatz werden die Bilder aus den Bildsequenzen einzeln klassifiziert. Anschließend wird den Sequenzen die Klasse zugeordnet, mit der die meisten Bilder aus der jeweiligen Sequenz klassifiziert wurden (relative Mehrheit). Für den zweiten Ansatz wird eine Kombination aus Bild- und Sequenzerkennung verwendet, um die Bildsequenzen als Ganzes zu klassifizieren. Die Architekturken für den jeweiligen Ansatz sind schematisch in Abbildung \ref{fig_ansaetze_architekturen} dargestellt und werden im Detail in den folgenden Absätzen erklärt. Dazu werden zu Beginn die Architekturen und Konfigurationen der verwendeten \acp{CNN} vorgestellt. Anschließend werden die verwendeten Fully-Connected- und \ac{LSTM}-Schichten mit den verwendeten Hyperparametern vorgestellt. Am Ende dieses Abschnitts werden alle Komponenten zusammengefügt und acht Architekturen vorgestellt.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Bilderkennung mit anschließendem Mehrheitsprinzip]{\includegraphics[scale=0.6]{images/ansatz_bilderkennung.pdf}} \\
\subfloat[Kombination aus Bild- und Sequenzerkennung]{\includegraphics[scale=0.6]{images/ansatz_bildsequenzerkennung.pdf}}
\end{tabular}
\caption{Zwei Ansätze für die Videoklassifizierung}
\label{fig_ansaetze_architekturen}
\end{figure}

% ===========================
\subsubsection{Auswahl von \aclp{CNN}}
% ===========================

Das Training mit \acp{CNN} hat in den vergangenen Jahren durch die Verfügbarkeit von großen Datenmengen, steigender Rechenleistung und die Wiederverwendung von bereits vortrainierten \acp{CNN} große Fortschritte gemacht. Die Verwendung von bereits trainierten Netzen wird Transferlernen (engl. transfer learning) genannt \cite{oquab2014transfer}. Die Idee von einem \ac{CNN} ist es, in den frühen Schichten sehr grundlegende Merkmale (engl. low-level features) und in späteren Schichten abstraktere Merkmale (engl. high-level features) zu extrahieren. Schließlich wird Bilder auf Basis dieser abstrakten Merkmale mithilfe von einer oder mehreren Fully-Connected-Schichten klassifiziert.

Diese Funktionsweise macht man sich beim Transferlernen zunutze und verwendet ein \ac{CNN} das bereits auf vielen Millionen Bildern trainiert wurde. Dieses \ac{CNN} hat dann bereits gelernt unterschiedliche Merkmale aus Bildern zu extrahieren. Um ein ein solches neuronales Netz für die eigene Arbeit verwenden zu können, muss man lediglich die letzten Fully-Connected-Schichten ersetzen und mit den eigenen Bilddaten und entsprechenden Klassen trainieren \cite{oquab2014transfer}. Mit diesem Verfahren kann viel Zeit und Rechenleistung gespart werden, weil die grundlegende Merkmalsextraktion nicht neu gelernt werden muss. Ein Datensatz der häufig für das Training von \ac{CNN}-Architekturen verwendet wird ist ImageNet. ImageNet umfasst 14.197.122 Bildern und 21.841 sogenannte Synonymmengen (engl. synonym sets) \cite{deng2009imagenet}. In Synonymmengen sind Wörter der gleichen Bedeutung zusammengefasst. Zusätzlich sind alle Wörter auch hierarchisch nach dem WordNet-Projekt eingeordnet. In dieser Arbeit werden \ac{CNN}-Architekturen verwendet, die mit dem ImageNet-Datensatz vortrainiert wurden.

Eine Übersicht von bekannten \ac{CNN}-Architekturen ist in Abbildung \ref{fig_comparison_cnns} dargestellt. In dieser Darstellung werden die neuronalen Netze anhand ihrer erreichten Genauigkeit (engl. accuracy) und der benötigten Operationen bei der Berechnung einer Klassifizierung eingeordnet \cite{canziani2016analysis}. Die Größe der Kreise zeigt die Anzahl der Parameter der jeweiligen Architektur. 

\begin{figure}[h]
\centering
\includegraphics[scale=0.3]{images/comparison_cnns.png}
\caption[Vergleich von vortrainierten \aclp{CNN}]{Vergleich von vortrainierten \aclp{CNN}, entnommen aus \cite{canziani2016analysis}}
\label{fig_comparison_cnns}
\end{figure}

Von diesen Architekturen wird in dieser Arbeit die Inception-V3-Architektur \cite{szegedy2016inception} als Basis verwendet. Diese Architektur hat wenige Parameter, was eine geringe Anzahl benötigter Operationen zur Folge hat, bei gleichzeitig sehr guter Genauigkeit \cite{canziani2016analysis}. Das Inception-V4 Modell ist zur Zeit dieser Arbeit noch nicht bei Keras verfügbar und kann daher nicht verwendet werden. Die zweite Architektur in dieser Arbeit ist die Xception-Architektur, welche auf den gleichen Prinzipien wie das Inception-V3-Netz entwickelt wurde. Diese Xception-Architektur wird in dieser Arbeit verwendet, weil sie dieselbe Anzahl Parameter wie die Inception-V3-Architektur hat, aber noch höhere Genauigkeit erzielt \cite{chollet2017xception}. In den folgenden Absätzen wird der Vorteil der Inception-Architekturen im Vergleich zu normalen tiefen \acp{CNN} erklärt und die beiden Architekturen Inception-V3 und Xception werden vorgestellt.

Einige Jahre waren tiefe \acp{CNN} mit vielen einfach gestapelten Convolution- und Pooling-Schichten, wie beispielsweise die VGG-16-Architektur \cite{simonyan2014vgg}, der Stand der Technik und erreichten gute Genauigkeiten bei der Klassifizierung von Bildern. Der Nachteil waren viele Parameter und ein sehr hoher Rechenaufwand \cite{canziani2016analysis}. Genau an diesen Problemen setzen Architekturen wie das Inception-V3 Netz an. Es werden verschiedenen Prinzipien angewendet, um den benötigten Rechenaufwand zu senken und dennoch die Genauigkeit zu verbessern. Dabei werden Convolution-Operationen nicht nur gestapelt, sondern auch parallel zu anderen Convolution-Operationen ausgeführt \cite{szegedy2016inception}.

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
\subfloat[Inception-Modul in dem ein 5x5-Filter mit zwei 3x3-Filter ersetzt wird]{\includegraphics[scale=0.3]{images/inc_fig_5.pdf}} &
\subfloat[Inception-Modul in dem ein 7x7-Filter mit einen 1x7-Filter gefolgt von einem 7x1 Filter ersetzt wird]{\includegraphics[scale=0.3]{images/inc_fig_6.pdf}} &
\subfloat[Inception-Modul in statt einer Pooling-Schicht mehrere parallele Convolution-Operationen ausgeführt werden]{\includegraphics[scale=0.3]{images/inc_fig_7.pdf}}
\end{tabular}
\caption[Inception-Module der Inception-V3 Architektur]{Inception-Module der Inception-V3 Architektur \cite{szegedy2016inception}}
\label{fig_inc_fig}
\end{figure}

Es wird argumentiert \cite{szegedy2016inception}, dass ein Filter mit der Dimension 5x5 durch zwei aufeinanderfolgende Filter der Dimension 3x3 ersetzt werden kann, was die benötigte Rechenleistung deutlich reduziert \cite{szegedy2016inception}. Diese Funktionalität wird in einem sogenannten Inception-Modul umgesetzt, was in Abbildung \ref{fig_inc_fig}a zu sehen ist. Ein weiteres Prinzip ist das Ersetzen eines nxn-Filters durch einen 1xn-Filter gefolgt von einem nx1-Filter. Damit wird der benötigte Rechenaufwand, bei gleichbleibender Leistung, um 33 Prozent gesenkt. Dieses Prinzip ist in Abbildung \ref{fig_inc_fig}b dargestellt. Für das dritte Prinzip wird argumentiert, dass bei einer starken Dimensionsreduzierung durch Pooling-Schichten viel Information aus dem Bild verloren gehen kann. Um dies zu verhindern, werden statt einer Pooling-Schicht mehrere parallele Convolution-Operationen ausgeführt. Dieses Prinzip wird mit dem Inception-Modul aus Abbildung \ref{fig_inc_fig}c umgesetzt.

Die Architektur des gesamten Netzes ist in Abbildung \ref{fig_inception_v3} dargestellt. In dieser Arbeit wird ein Inception-V3 Netz verwendet, das mit dem Datensatz ImageNet \cite{deng2009imagenet} trainiert wurde. Außerdem werden die letzten Schichten der Inception-V3-Architektur (in der Abbildung der \textit{final part}) am Ende des Netzes entfernt, weil sie für die Klassifizierung des ImageNet Datensatzes angepasst sind. Sie werden durch anderen Schichten ersetzt, die in den nachfolgenden Absätzen erklärt werden und der Klassifizierungaufgabe von dieser Arbeit angepasst sind.

\begin{figure}[h]
\centering
\includegraphics[scale=0.4]{images/inception_v3.png}
\caption[Architektur des Inception-V3 Netzes]{Architektur des Inception-V3 Netzes, entnommen aus \cite{google2018inceptionv3}}
\label{fig_inception_v3}
\end{figure}

Die Grundidee bestehende Filter zu vereinfachen und um eine Dimension zu reduzieren (7x7 -> 1x7 und 7x1), wird mit der Xception-Architektur aufgegriffen und noch weiter vorangetrieben \cite{chollet2017xception}. Die Idee ist es, mit Inception-Modulen die räumliche Convolution-Operation (engl. spatial convolution) und die Convolution-Operation über die Kanäle (engl. depthwise convolution) zu trennen. Das heißt, dass es einen Filter mit den Dimensionen 1x1xc gibt, der auf ein Bild oder eine Feature Map der Tiefe $c$ angewendet wird, gefolgt von einem Filter mit den Dimensionen 3x3x1, der auf jede Schicht der Feature Map angewendet wird. Diese getrennten Convolution-Operationen (engl. depthwise separable convolution) sind in dem Inception-Modul in Abbildung \ref{fig_inception_extreme} dargestellt.

\begin{figure}[h]
\centering
\includegraphics[scale=0.5]{images/inception_extreme.pdf}
\caption[Inception-Modul der Xception-Architektur]{Inception-Modul der Xception-Architektur \cite{chollet2017xception}}
\label{fig_inception_extreme}
\end{figure}

Die gesamte Xception-Architektur besteht aus mehreren dieser getrennten Convolution-Operationen und ist in Abbildung \ref{fig_xception} dargestellt. Wie die Gewichte des Inception-V3 Netzes, sind auch die Gewichte des Xception Netzes mit dem Imagenet Datensatz vortrainiert. Auch hier werden die letzten Fully-Connected-Schichten entfernt und mit passenden Schichten für die Klassifizierung in dieser Arbeit ersetzt.

\begin{figure}[h]
\centering
\includegraphics[scale=0.35]{images/xception.png}
\caption[Xception-Architektur]{Xception-Architektur, entnommen aus \cite{chollet2017xception}}
\label{fig_xception}
\end{figure}

% ===========================
\subsubsection{Fully-Connected- und \ac{LSTM}-Schicht}
% ===========================

Eine Fully-Connected-Schicht ist, wie in Abschnitt \ref{grundlagen_nn_entwicklung} beschrieben, eine Schicht mit $n$ Neuronen in einem Multilayer-Perzeptron. Diese Schicht muss mit einer Anzahl Neuronen $n$ und einer Aktivierungsfunktion $\varphi$ konfiguriert werden. Die Ergebnisschicht in einem \ac{KNN} ist immer eine Fully-Connected-Schicht. In dieser Arbeit werden in jeder Architektur zwei dieser Schichten verwendet, eine nach dem vortrainierten \ac{CNN} bzw. der \ac{LSTM}-Schicht und die andere als Ergebnisschicht. Die erste Fully-Connected Schicht besteht in dieser Arbeit immer aus 128 Neuronen und der \ac{ReLU}-Aktivierungsfunktion \cite{nair2010rectified}. In der Ergebnisschicht werden alle zuvor extrahierten Merkmale auf die Anzahl der Klassen heruntergebrochen. Daher besteht die Ergebnisschicht aus der gleichen Anzahl Neuronen wie es Klassen gibt. Als Aktivierungsfunktion für ein Multiklassen-Problem ($C>2$), wie in dieser Arbeit, wird häufig die Softmax-Funktion verwendet \cite{bishop2006pattern}.

Wie in Abschnitt \ref{grundlagen_nn_rnn} beschrieben, werden \ac{LSTM}-Schichten für die Sequenzerkennung eingesetzt. Da die \ac{LSTM}-Schicht in dieser Arbeit nach der Merkmalsextraktion mit einem \ac{CNN} verwendet wird, ist in Keras eine sogenannte \textit{TimeDistributed}-Schicht nötig, um die Merkmale aller Bilder einer Sequenz der \ac{LSTM}-Schicht übergeben zu können. Diese Umhüllung (engl. wrapper) hat keine zu konfigurierenden Parameter. Die \ac{LSTM}-Schicht wird mit der Dimension $units = 256$ und der \ac{tanh}-Aktivierungsfunktion konfiguriert.

% ===========================
\subsubsection{Regularisierung}
% ===========================

Das Ziel jedes \acp{KNN} ist es, nach dem Training gute Ergebnisse auf bisher unbekannten Daten zu erzielen. Dies wird Generalisierbarkeit eines Netzes genannt \cite{bishop2006pattern}. Regularisierungsmethoden werden oft während des Trainings eingesetzt, um die Generalisierbarkeit nach dem Training zu erhöhen. In dieser Arbeit werden die Methoden Dropout \cite{hinton2012improving} und Early Stopping \cite{prechelt1998early} verwendet.

\begin{figure}[h]
\centering
\begin{tabular}{c@{\hskip 1.5cm}c}
\subfloat[\ac{KNN} ohne Dropout]{\includegraphics[scale=1.5]{images/dropout_1.pdf}} &
\subfloat[\ac{KNN} mit Dropout in der mittleren Schicht mit der Rate $r=0,5$]{\includegraphics[scale=1.5]{images/dropout_2.pdf}}
\end{tabular}
\caption[Regularisierung mit Dropout]{Regularisierung mit Dropout \cite{srivastava2014dropout}}
\label{fig_dropout}
\end{figure}

Dropout ist eine Methode, um beim Training von \acp{KNN} Überanpassung (engl. Overfitting) zu vermeiden und insgesamt bessere Leistung bei der Kombination von verschiedenen Architekturen zu erreichen \cite{hinton2012improving}. Die Grundidee von Dropout ist es, die Komplexität eines \acp{KNN} zu reduzieren. Die Komplexität eines \ac{KNN} beschreibt die Anzahl der zu trainierenden Parameter. Je mehr Parameter ein \ac{KNN} besitzt, desto komplexer ist es. Dropout reduziert die Komplexität indem bei jedem Trainingsschritt zufällig einige Neuronen und die zugehörigen Kanten im Netz, die Parameter, entfernt. Dafür wird einer Schicht eine Rate $r$ zugeordnet. Diese Rate $r$ gibt an, welcher Anteil der Neuronen aus dieser Schicht zufällig entfernt wird. Bei $r=0,5$ werden beispielsweise 50 Prozent aller Neuronen in jedem Trainingsschritt zufällig entfernt. Das Ergebnis ist, dass das \ac{KNN} nicht zu abhängig von einzelnen Neuronen oder Gewichten wird und insgesamt bessere Ergebnisse erzielt werden können \cite{srivastava2014dropout}. In dieser Arbeit wird Dropout mit der Rate $r=0,5$ verwendet. Obwohl Dropout keine eigenständige Schicht ist, wird es in Keras als Schicht modelliert. Dabei bezieht sich Dropout aber stets auf die jeweilige Schicht davor. Ein Beispiel von Dropout ist in Abbildung \ref{fig_dropout} dargestellt.

Early Stopping ist eine Methode, bei der das Training eines \acp{KNN} abgebrochen wird, wenn die Genauigkeit oder der Fehler sich nicht weiter verbessern \cite{prechelt1998early}. In dieser Arbeit wird die Genauigkeit der Validierungsdaten (engl. validation accuracy) als Metrik verwendet und das Training wird beendet, wenn sich diese Metrik innerhalb von 20 Epochen nicht mindestens um den Wert 0,01 verbessert.


% ===========================
\subsection{Wahl der Komponenten}
\label{umsetzung_training_experimente}
% ===========================

Bei der Konfiguration von \acp{KNN} und den einzelnen Schichten können viele unterschiedliche Komponenten ausgewählt werden. Da es sich bei dieser Arbeit um einen Proof-of-Concept handelt, werden oftmals die Standardkonfigurationen von Keras verwendet. Diese Standardkonfigurationen basieren nach Chollet stets auf aktuellen Forschungsergebnissen und dem Stand der Technik bei \acp{KNN} \cite{chollet2015keras}. Diese Vorgehensweise erlaubt einerseits eine schnelle Umsetzung des Proof-of-Concept dieser Arbeit und bietet andererseits in nachfolgenden Arbeiten viel Potential die Ergebnisse weiter zu verbessern.

Vor jedem Training werden die \acp{KNN} mit einem Optimierer (engl. optimizer) und einer Fehlerfunktion (engl. loss function) kompiliert. Als Optimierer wird in dieser Arbeit Adam (adaptive moment estimation) verwendet. Dieser Optimierer vereint die Vorteile von den Optimierern AdaGrad (adaptive gradient algorithm) und RMSProp (root mean square propagation). Das Grundprinzip ist es, die Lernrate dynamisch anzupassen, was zu weniger benötigtem Speicher und einer hohen Konvergenzrate führt \cite{kingma2014adam}.

 Neben der mittleren quadratischen Abweichung, die in Abschnitt \ref{grundlagen_nn_entwicklung} vorgestellt wurde, gibt es weitere Fehlerfunktionen. In dieser Arbeit wird die Kreuzentropie (engl. cross entropy) für Mehrklassenprobleme mit der folgenden Fehlerfunktion verwendet \cite{bishop2006pattern}:

\begin{equation}
E(w_1, ..., w_K) = -\sum _{n=1}^N \sum _{k=1}^K t_{nk} * ln(y_{nk})
\end{equation}

Dabei beschreiben $w_1, ..., w_K$ die zu optimierenden Gewichte, $N$ ist die Anzahl der Trainingsdaten und $K$ die Anzahl der Klassen. $t_{nk}$ gibt mit dem Wert $1$ an, ob der Dateninput $n$ zur Klasse $k$ gehört. Wenn $n$ nicht zur Klasse $k$ gehört, nimmt $t_{nk}$ den Wert $0$ an. $y_{nk}$ ist die berechnete Wahrscheinlichkeit des Klassifikators, dass der Dateninput $n$ zur Klasse $k$ gehört.

Das Ziel der Experimente ist das Erreichen einer möglichst hohen Genauigkeit (engl. accuracy) bei der Klassifizierung von Bildsequenzen. Bei Mehrklassenproblemen (engl. multi-class problem) werden häufig die Top-1 und Top-5 Genauigkeit bzw. der Top-1 und Top-5 Fehler untersucht \cite{szegedy2016inception, simonyan2014vgg, chollet2017xception}. Das Ergebnis einer Multiklassen-Klassifikation für jeden Input ist ein Vektor mit Wahrscheinlichkeiten für jede Klasse. Bei der Top-1 Genauigkeit bzw. dem Top-1 Fehler wird jeweils nur die Klasse mit der höchsten Genauigkeit betrachtet und mit der richtigen Klasse abgeglichen. Bei der Top-5 Genauigkeit bzw. dem Top-5 Fehler werden jeweils die fünf Klassen mit den höchsten Wahrscheinlichkeit mit der richtigen Klasse abgeglichen. Da in dieser Arbeit nur insgesamt fünf Klassen betrachtet werden, wird nur die Top-1 Genauigkeit als Metrik verwendet.

Anhand dieser Metrik werden die unterschiedlichen Architekturen aus dem Abschnitt \ref{umsetzung_training_architektur} mit 100 Epochen und Early Stopping trainiert und dann getestet. Aus der Variation von drei Komponenten mit jeweils zwei Variationen ergeben sich acht Architekturen. Die Komponenten mit ihren Variationen sind in Tabelle \ref{tab_parameter} zusammengefasst.

\begin{table}[h]
\small
\centering
\def\arraystretch{1.4}
\begin{tabular}{l l}
\textbf{Komponenten} & \textbf{Variationen} \\
\hline
Prinzip der Klassifizierung & einzelne Bilder, Bildsequenzen \\
\acl{CNN} & Inception-V3, Xception \\
Regularisierung & ohne Dropout, mit Dropout \\
\hline
\end{tabular}
\caption{Komponenten die in den Experimenten variiert werden}
\label{tab_parameter}
\end{table}


% ===========================
\subsubsection{Komponenten zusammenfügen}
% ===========================

Auf Basis der oben beschriebenen Komponenten werden acht verschiedene \ac{KNN}-Architekturen erstellt. Dabei wird zwischen den Architekturen für die Klassifizierung einzelner Bilder und den Architekturen für die Klassifizierung von Bildsequenzen unterschieden. Beide grundlegenden Architekturen sind schematisch in Abbildung \ref{fig_architekturen_grundlegend} dargestellt.

\begin{figure}[h]
\centering
\begin{tabular}{c}
\subfloat[Architektur für die Klassifizierung einzelner Bilder]{\includegraphics[scale=0.4]{images/architektur_1.png}} \\
\subfloat[Architektur für die Klassifizierung von Bildsequenzen]{\includegraphics[scale=0.4]{images/architektur_2.png}}
\end{tabular}
\caption{Grundlegende \ac{KNN}-Architekturen in dieser Arbeit}
\label{fig_architekturen_grundlegend}
\end{figure}

Ausgehend von der Darstellung in Abbildung \ref{fig_architekturen_grundlegend} wird bei den Architekturen für die Klassifizierung von einzelnen Bildern zwischen den zwei \acp{CNN} Inception-V3 \cite{szegedy2016inception} und Xception \cite{chollet2017xception} und zwischen einer Version mit und ohne Dropout \cite{srivastava2014dropout} unterschieden. Damit ergeben sich vier verschiedene Architekturen. Als Basis wird dafür bei jeder Architektur eines der oben beschriebenen \acp{CNN} verwendet. Nach den Convolution- und Pooling-Schichten des jeweiligen \acp{CNN} wird eine Fully-Connected-Schicht mit 128 Neuronen und der \ac{ReLU}-Aktivierungsfunktion eingefügt. Diese Schicht wird in zwei der vier Architekturen mit Dropout, mit $r=0,5$, konfiguriert. Abschließend wird eine weitere Fully-Connected-Schicht mit fünf Neuronen, was der Anzahl der zu klassifizierenden Szenen entspricht, und der Softmax-Aktivierungsfunktion angehängt. Diese vier Konfigurationen sind in der Tabelle \ref{tab_architekturen_bild} mit den jeweiligen Komponenten und Hyperparametern zusammengefasst.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Architekturen}} \\
\hline
\textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} \\
\hline
\hline
\multicolumn{4}{|c|}{Input shape: 299x299x3} \\
\hline
\multicolumn{2}{|c|}{Inception-V3} & \multicolumn{2}{|c|}{Xception} \\
\multicolumn{2}{|c|}{Output shape: 2048} & \multicolumn{2}{|c|}{Output shape: 2048} \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht} \\
\multicolumn{4}{|c|}{$units=128$, $activation=\text{\ac{ReLU}}$} \\
\multicolumn{4}{|c|}{Output shape: 128} \\
\hline
ohne Dropout & Dropout & ohne Dropout & Dropout \\
- & $r=0,5$ & - & $r=0,5$ \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht (Ergebnisschicht)} \\
\multicolumn{4}{|c|}{$units=5$, $activation=Softmax$} \\
\multicolumn{4}{|c|}{Output shape: 5} \\
\hline
\end{tabular}
\caption{\ac{KNN}-Architekturen für die Klassifizierung von Bildsequenzen}
\label{tab_architekturen_bild}
\end{table}

Analog zu den Architekturen für die Klassifizierung von einzelnen Bildern, werden bei den Architekturen für die Klassifizierung von Bildsequenzen ebenfalls die zwei verschiedene \ac{CNN}-Architekturen variiert. Der Unterschied ist, dass diese \acp{CNN} jeweils in eine \textit{TimeDistributed}-Schicht eingehüllt werden, um danach eine \ac{LSTM}-Schicht mit 256 Neuronen anzuhängen. Danach wird wie bei den anderen Architekturen eine Fully-Connected-Schicht mit 128 Neuronen und der \ac{ReLU}-Aktivierungsfunktion eingefügt und bei zwei von vier Architekturen mit der Regularisierungsmethode Dropout ($r=0,5$) konfiguriert. Die Ergebnisschicht ist ebenfalls identisch mit 5 Neuronen und der Softmax-Aktivierungsfunktion. Die resultierenden vier Architekturen für die Klassifizierung von Bildsequenzen sind in Tabelle \ref{tab_architekturen_video} mit ihren Komponenten und Hyperparametern zusammengefasst.

\begin{table}[h]
\centering
\def\arraystretch{1.4}
\begin{tabular}{|c|c|c|c|}
\hline
\multicolumn{4}{|c|}{\textbf{Architekturen}} \\
\hline
\textbf{E} & \textbf{F} & \textbf{G} & \textbf{H} \\
\hline
\hline
\multicolumn{4}{|c|}{Input shape: 15x299x299x3} \\
\hline
\multicolumn{2}{|c|}{\textit{TimeDistributed} mit Inception-V3} & \multicolumn{2}{|c|}{\textit{TimeDistributed} mit Xception} \\
\multicolumn{2}{|c|}{Output shape: 15x2048} & \multicolumn{2}{|c|}{Output shape: 15x2048} \\
\hline
\multicolumn{4}{|c|}{\ac{LSTM}-Schicht} \\
\multicolumn{4}{|c|}{$units=256$, $activation=\text{\ac{tanh}}$} \\
\multicolumn{4}{|c|}{Output shape: 256} \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht} \\
\multicolumn{4}{|c|}{$units=128$, $activation=\text{\ac{ReLU}}$} \\
\multicolumn{4}{|c|}{Output shape: 128} \\
\hline
ohne Dropout & Dropout & ohne Dropout & Dropout \\
- & $r=0,5$ & - & $r=0,5$ \\
\hline
\multicolumn{4}{|c|}{Fully-Connected-Schicht (Ergebnisschicht)} \\
\multicolumn{4}{|c|}{$units=5$, $activation=Softmax$} \\
\multicolumn{4}{|c|}{Output shape: 5} \\
\hline
\end{tabular}
\caption{\ac{KNN}-Architekturen für die Klassifizierung einzelner Bilder}
\label{tab_architekturen_video}
\end{table}













 


 
 

